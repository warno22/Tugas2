{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NAMA : Abd Ghofar Suwarno NIM : 170411100015 Tugas Pertama Penambangan Data Mean (Nilai Rata-Rata) Yang dimaksud mean atau nilai rata-rata adalah jumlah seluruh data dibagi dengan banyaknya data. Cara menghitung mean adalah dengan menggunakan rumus menghitung nilai rata-rata dari sekumpulan data sebagai berikut. Median (Nilai Tengah) Yang dimaksud median adalah nilai data yang letaknya di tengah dari data yang telah diurutkan dari nilai terkecil sampai nilai terbesar. Biasanya median diberi simbol Me. Cara Menghitung Median dengan Banyaknya Data Ganjil Pada data yang banyaknya ganjil maka ada satu data di tepat tengah data yang telah diurutkan. Jika banyaknya data ganjil, maka median adalah data yang letaknya tepat di tengah sekumpulan data yang telah diurutkan tersebut . Misalkan kita akan menghitung median dari 11 data berikut: 21, 27, 23, 25, 21, 28, 24, 27, 26, 25, 21 Urutan data dari yang terkecil: 21, 21, 21, 23, 24, 25 , 25, 26, 27, 27, 28 Median adalah data yang di tengah urutan yaitu 25 Cara Menghitung Median dengan Banyaknya Data Genap Pada data yang banyaknya genap maka ada dua data di tepat tengah data yang telah diurutkan. Jika banyaknya data genap, maka median data adalah rata-rata kedua data yang letaknya tepat di tengah sekumpulan data yang telah diurutkan tersebut . Misalkan kita akan menghitung median dari 12 data berikut: 24, 33, 47, 60, 30, 24, 25, 35, 49, 41, 52, 58, Urutan data dari yang terkecil: 24, 24, 25, 30, 33, 35 , 41 , 47, 49, 52, 58, 60 Median adalah rata-rata dua data di tengah = (35+41)/2 = 76/2 = 38 Modus Yang dimaksud modus adalah data yang paling sering muncul atau yang memiliki frekuensi terbanyak dari sekumpulan data. Biasanya modus diberi simbol Mo. Cara menentukan modus adalah dengan menghitung frekuensi semua data lalu memilih data yang frekuensi munculnya terbesar. Misalkan kita akan menghitung modus dari 10 data berikut: 7, 9, 8, 10, 6, 8, 6, 8, 7, 8 Dari data tersebut angka 8 muncul paling sering yaitu empat kali, angka 6 dan 7 muncul dua kali, sedangkan angka 9 dan 10 muncul sekali. Maka modusnya adalah 8. Kuartil Yang dimaksud dengan kuartil adalah data yang membagi posisi sekumpulan data yang telah diurutkan menjadi empat bagian. Dalam satu urutan data terdapat 3 kuartil yaitu kuartil bawah, kuartil tengah, dan kuartil atas. Cara menentukan kuartil adalah sebagai berikut. Kuartil bawah adalah data pada posisi 1/4 dari kumpulan data yang telah diurutkan. Kuartil bawah disimbolkan dengan Q1. Kuartil tengah adalah data pada posisi 2/4 dari kumpulan data yang telah diurutkan. Kuartil tengah sama dengan median. Kuartil tengah disimbolkan dengan Q2. Kuartil atas adalah data pada posisi 3/4 dari kumpulan data yang telah diurutkan. Kuartil atas disimbolkan dengan Q3. Posisi ketiga kuartil ditentukan dari rumus berikut. Posisi Qi = i(n+1)/4 i = indeks kuartil yaitu 1, 2, 3 dan n = banyaknya data Misalkan kita akan menentukan kuartil bawah, tengah, dan kuartil atas dari 15 data berikut: 11, 24, 12, 15, 12, 18, 22, 25, 26, 27, 17, 22, 24, 19, 12. Urutan data dari yang terkecil: 11, 12, 12, 12, 15, 17, 18, 19, 22, 22, 24, 24, 25, 26, 27 Posisi ketiga kuartil adalah sebagai berikut Posisi Q1 = 1.(15+1)/4 = (16)/4 = 4 (data urutan ke 4) Posisi Q2 = 2. (15+1)/4 = 2(16)/4 = 8 (data urutan ke 4) Posisi Q3 = 3. (15+1)/4 = 3(16)/4 = 12 (data urutan ke 4) Berdasarkan posisi kuartil pada urutan data maka dapat ditentukan ketiga kuartil 11, 12, 12, 12 , 15, 17, 18, 19 , 22, 22, 24, 24 , 25, 26, 27 Jadi : kuartil bawah adalah 12 Kuartil tengah = median = 19 Kuartil atas = 24 Ragam (variansi) Ragam adalah rata-rata selisih kuadrat antara nilai-nilai individual dengan nilai tengahnya. Maka untuk mencari ragam adalah : jadi, ragam dari table nilai diatas adalah 9,75 Simpangan Baku Simpangan baku adalah ukuran sebaran statistic yang paling lazim, atau didefinisikan juga sebagaia karkuadratvarians. Maka untuk mencari simpangan baku adalah : Jadi simpangan bakunya adalah 3,12 Kemiringan kurva (Skewnes) Merupakan derajat ketidak simetrian (keasimetrian), atau dapat juga disefinisikan sebagai penyimpangan dari kesimetrian dari suatu distribusi. Maka untuk mencari nilai skewnes adalah Karena nilai SK nya 0,09 ( SK > 0 ) maka kurvanya mengarah ke kanan Code Yang saya telah buat untuk mencari mean , median , modus , standar deviasi , varian , skewnes , dan kuartil (1 , 2 , 3 ) : import numpy as ip import seaborn as sea import matplotlib.pyplot as mat from scipy import stats import pandas as data for i in range(4): cek1=data.read_csv(\"warno.csv\",usecols=[i]) c= cek1.count() print(\"Jumlah sel data = \",c) print(\" \") m= cek1.mean() print(\"mean = \",m) print(\" \") e= cek1.median() print(\"median = \",e) print(\" \") o= cek1.mode() print(\"modus = \",o) print(\" \") s=cek1.std() print(\"standar deviasi =\",s) print(\" \") v= cek1.var() print(\"varian = \",v) print(\" \") w= cek1.skew() print(\"skewwnes = \",s) print(\" \") Q1= cek1.quantile(0.25) print(\"Kuartil awal = \",Q1) print(\" \") Q2= cek1.quantile(0.50) print(\"Kuartil tengah = \",Q2) print(\" \") Q3= cek1.quantile(0.75) print(\"Kuartil tengah = \",Q3) print(\" \") HASIL RUN : Jumlah sel data = Tinggi Badan 34 dtype: int64 mean = Tinggi Badan 173.117647 dtype: float64 median = Tinggi Badan 172.5 dtype: float64 modus = Tinggi Badan 0 180 standar deviasi = Tinggi Badan 7.057189 dtype: float64 varian = Tinggi Badan 49.803922 dtype: float64 skewwnes = Tinggi Badan 7.057189 dtype: float64 Kuartil awal = Tinggi Badan 167.5 Name: 0.25, dtype: float64 Kuartil tengah = Tinggi Badan 172.5 Name: 0.5, dtype: float64 Kuartil tengah = Tinggi Badan 180.0 Name: 0.75, dtype: float64 Jumlah sel data = Berat Badan 34 dtype: int64 mean = Berat Badan 54.323529 dtype: float64 median = Berat Badan 56.0 dtype: float64 modus = Berat Badan 0 69 standar deviasi = Berat Badan 12.011915 dtype: float64 varian = Berat Badan 144.286096 dtype: float64 skewwnes = Berat Badan 12.011915 dtype: float64 Kuartil awal = Berat Badan 46.0 Name: 0.25, dtype: float64 Kuartil tengah = Berat Badan 56.0 Name: 0.5, dtype: float64 Kuartil tengah = Berat Badan 64.75 Name: 0.75, dtype: float64 Jumlah sel data = Umur 34 dtype: int64 mean = Umur 20.676471 dtype: float64 median = Umur 21.0 dtype: float64 modus = Umur 0 22 standar deviasi = Umur 1.821098 dtype: float64 varian = Umur 3.316399 dtype: float64 skewwnes = Umur 1.821098 dtype: float64 Kuartil awal = Umur 19.0 Name: 0.25, dtype: float64 Kuartil tengah = Umur 21.0 Name: 0.5, dtype: float64 Kuartil tengah = Umur 22.0 Name: 0.75, dtype: float64 Jumlah sel data = Nilai 34 dtype: int64 mean = Nilai 63.058824 dtype: float64 median = Nilai 65.0 dtype: float64 modus = Nilai 0 69 standar deviasi = Nilai 19.257334 dtype: float64 varian = Nilai 370.84492 dtype: float64 skewwnes = Nilai 19.257334 dtype: float64 Kuartil awal = Nilai 50.0 Name: 0.25, dtype: float64 Kuartil tengah = Nilai 65.0 Name: 0.5, dtype: float64 Kuartil tengah = Nilai 75.5 Name: 0.75, dtype: float64 Tugas 2 Menghitung jarak pada data Pengertian, Rumus , dan Penjelasan Menghitung Jarak Pada Data A. Cara Kerja Algoritma K-Nearest Neighbors (KNN) K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak. Klasifikasi Terdekat (Nearest Neighbor Classification) Data baru yang diklasifikasi selanjutnya diproyeksikan pada ruang dimensi banyak yang telah memuat titik-titik c data pembelajaran. Proses klasifikasi dilakukan dengan mencari titik c terdekat dari c-baru ( nearest neighbor ) . Teknik pencarian tetangga terdekat yang umum dilakukan dengan menggunakan formula jarak euclidean . Berikut beberapa formula yang digunakan dalam algoritma knn. Euclidean Distance Jarak Euclidean adalah formula untuk mencari jarak antara 2 titik dalam ruang dua dimensi. Hamming Distance Jarak Hamming adalah cara mencari jarak antar 2 titik yang dihitung dengan panjang vektor biner yang dibentuk oleh dua titik tersebut dalam block kode biner. Manhattan Distance Manhattan Distance atau Taxicab Geometri adalah formula untuk mencari jarak d antar 2 vektor p,q pada ruang dimensi n . Minkowski Distance Minkowski distance adalah formula pengukuran antar 2 titik pada ruang vektor normal yang merupakan hibridisasi yang mengeneralisasi euclidean distance dan mahattan distance. Diberikan dua obyek yang diwakili oleh tuple (a,b,c,d,e,f) dan (f,g,h,i,j). Kita misalkan obyek pertama dengan i = (a,b,c,d,e) dan obyek kedua dengan j = (f,g,h,i,j) berikut nilai-nilainya : i j a = 41 91=3731 f = 41 80=3280 b = 41 92=3772 g = 41 81=3321 c = 41 93=3813 h = 41 82=3362 d = 41 94=3854 i = 41 83=3403 e = 41 95=3895 j = 41 84=3444 memanggil fungsi yang aa pada library scipy dan membaca file exel yang telah dibuat. from scipy import stats import pandas as pd pd.read_csv(\"Tugas2.csv\") Hasil Run 14 8 440 215 4312 8.5 70 1 plymouth fury iii 0 14.0 8 455.0 225 4425 10.0 70 1 pontiac catalina 1 15.0 8 390.0 190 3850 8.5 70 1 amc ambassador dpl 2 15.0 8 383.0 170 3563 10.0 70 1 dodge challenger se 3 14.0 8 340.0 160 3609 8.0 70 1 plymouth 'cuda 340 4 15.0 8 400.0 150 3761 9.5 70 1 chevrolet monte carlo ... ... ... ... ... ... ... ... ... ... 385 27.0 4 140.0 86 2790 15.6 82 1 ford mustang gl 386 44.0 4 97.0 52 2130 24.6 82 2 vw pickup 387 32.0 4 135.0 84 2295 11.6 82 1 dodge rampage 388 28.0 4 120.0 79 2625 18.6 82 1 ford ranger 389 31.0 4 119.0 82 2720 19.4 82 1 chevy s-10 390 rows \u00d7 9 columns Mencari jarak dengan methode Euclidia Distance dan methode manahattanDistance from scipy import stats import pandas as pd import math df=pd.read_csv(\"servo.csv\") def Euclidean(i,j): hasil = 0 for ii in range(len(df)-1): rumus = (i[ii]-j[ii])**2 hasil = hasil + rumus return math.sqrt(hasil) print(\"M = 2\") print(\"jarak kedua objek adalah :\",Euclidean(df['vgain'],df['class'])) def Manahattan(i,j): hasil = 0 for kk in range(len(df)-1): rumus = i[kk]-j[kk] hasil = hasil + rumus return hasil print(\"M = 1\") print(\"jarak kedua objek adalah :\",Manahattan(df['vgain'],df['class'])) HASIL RUN M = 2 jarak kedua objek adalah : 34.30324786357754 M = 1 jarak kedua objek adalah : 187.72495382000025 TUGAS 3 feature selection Entropy menggunakan tabel frekuensi dari satu atribut Information Gain Ada banyak teknik untuk menyeleksi fitur. Information Gain bekerja dengan mendeteksi fitur yang paling banyak memiliki informasi berdasarkan kelas tertentu dengan menghitung nilai Entropy-nya Rumus untuk mencari Entropy Target adalah: Entropy(S): c\u2211i\u2212Pilog2PiEntropy(S): \u2211ic\u2212Pilog2\u2061Pi Dimana c adalah jumlah nilai yang ada pada kelas kategori dan Pi adalah jumlah sampel untuk kelas i import numpy as np import seaborn as sea import matplotlib.pyplot as mat from scipy import stats import pandas as data import math as rumus sample = data.read_csv(\"feture selection.csv\") print(sample) def entropy(b,a): result = -(a*rumus.log2(a))-(b*rumus.log2(b)) return result def play(yes,no): dataclass = [] for i in sample['play']: dataclass.append(i) data = len(dataclass) tempyes = yes/data tempno = no/data result = entropy(tempyes,tempno) return result mother_value=play(9,5) Entropy menggunakan tabel frekuensi dengan dua attribut def Entropy(b,a,c): result = -(a/c*rumus.log2(a/c))-(b/c*rumus.log2(b/c)) return result untuk outlook def outlook(sunny_yes,sunny_no,overcast_yes,overcast_no,rainy_yes,rainy_no): data_sunny = sunny_yes + sunny_no data_overcast = overcast_yes +overcast_no data_rainy = rainy_yes + rainy_no data = data_sunny + data_overcast + data_rainy E_sunny = Entropy(sunny_no,sunny_yes,data_sunny) E_overcast = 0 E_rainy = Entropy(rainy_no,rainy_yes,data_rainy) count = (data_sunny/data)*E_sunny + (data_overcast/data) *0 + (data_rainy/data)*E_rainy return count untuk temperature def temperature(hot_yes,hot_no,mild_yes,mild_no,cool_yes,cool_no): data_hot = hot_yes + hot_no data_mild = mild_yes +mild_no data_cool = cool_yes + cool_no data = data_hot + data_mild + data_cool E_hot = Entropy(hot_no,hot_yes,data_hot) E_mild = Entropy(mild_no,mild_yes,data_mild) E_cool = Entropy(cool_no,cool_yes,data_cool) count = (data_hot/data)*E_hot + (data_mild/data) *E_mild + (data_cool/data)*E_cool return count untuk Humidity def Humidity(high_yes,high_no,normal_yes,normal_no): data_high = high_yes + high_no data_normal = normal_yes +normal_no data = data_high + data_normal E_high = Entropy(high_yes,high_no,data_high) E_normal = Entropy(normal_yes,normal_no,data_normal) count = (data_high/data)*E_high + (data_normal/data) *E_normal return count untuk windy def windy(False_yes,False_no,True_yes,True_no): data_False = False_yes + False_no data_True = True_yes + True_no data = data_False + data_True E_false = Entropy(False_yes,False_no,data_False) E_true = Entropy(True_yes,True_no,data_True) count = (data_False/data)*E_false + (data_True/data) *E_true return count def rdoutlook(): count = 0 temp=[] rainy_yes = [] rainy_no = [] overcast_yes = [] overcast_no =[] sunny_yes = [] sunny_no =[] for v in sample['play']: if (v == \"yes\"): temp.append(\"yes\") else: temp.append(\"no\") for i in sample['outlook']: if (i == \"rainy\" and temp[count] == \"yes\"): rainy_yes.append(i) elif (i == \"rainy\" and temp[count] == \"no\"): rainy_no.append(i) elif (i == \"overcast\" and temp[count] == \"yes\"): overcast_yes.append(i) elif (i == \"overcast\" and temp[count] == \"no\"): overcast_no.append(i) elif (i == \"sunny\" and temp[count] == \"yes\"): sunny_yes.append(i) elif (i == \"sunny\" and temp[count] == \"no\"): sunny_no.append(i) count += 1 Voutlook=outlook(len(rainy_yes),len(rainy_no),len(overcast_yes),len(overcast_no),len(sunny_yes),len(sunny_no)) gain_outlook = mother_value - Voutlook return gain_outlook def rdtemperature(): count = 0 temp=[] hot_yes = [] hot_no = [] mild_yes = [] mild_no =[] cool_yes = [] cool_no =[] for v in sample['play']: if (v == \"yes\"): temp.append(\"yes\") else: temp.append(\"no\") for i in sample['temperature']: if (i == \"hot\" and temp[count] == \"yes\"): hot_yes.append(i) elif (i == \"hot\" and temp[count] == \"no\"): hot_no.append(i) elif (i == \"mild\" and temp[count] == \"yes\"): mild_yes.append(i) elif (i == \"mild\" and temp[count] == \"no\"): mild_no.append(i) elif (i == \"cool\" and temp[count] == \"yes\"): cool_yes.append(i) elif (i == \"cool\" and temp[count] == \"no\"): cool_no.append(i) count += 1 Vtemperature=temperature(len(hot_yes),len(hot_no),len(mild_yes),len(mild_no),len(cool_yes),len(cool_no)) gain_temperature = mother_value - Vtemperature return gain_temperature def rdhumidity(): count = 0 temp=[] high_yes = [] high_no = [] normal_yes = [] normal_no =[] for v in sample['play']: if (v == \"yes\"): temp.append(\"yes\") else: temp.append(\"no\") for i in sample['humidity']: if (i == \"high\" and temp[count] == \"yes\"): high_yes.append(i) elif (i == \"high\" and temp[count] == \"no\"): high_no.append(i) elif (i == \"normal\" and temp[count] == \"yes\"): normal_yes.append(i) elif (i == \"normal\" and temp[count] == \"no\"): normal_no.append(i) count += 1 Vhumidity=Humidity(len(high_yes),len(high_no),len(normal_yes),len(normal_no)) gain_humidity = mother_value - Vhumidity return gain_humiditydef rdwindy(): count = 0 temp=[] false_yes = [] false_no = [] true_yes = [] true_no =[] for v in sample['play']: if (v == \"yes\"): temp.append(\"yes\") else: temp.append(\"no\") for i in sample['windy']: if (i == False and temp[count] == \"yes\"): false_yes.append(i) elif (i == False and temp[count] == \"no\"): false_no.append(i) elif (i == True and temp[count] == \"yes\"): true_yes.append(i) elif (i == True and temp[count] == \"no\"): true_no.append(i) count += 1 Vwindy=windy(len(false_yes),len(false_no),len(true_yes),len(true_no)) gain_windy = mother_value - Vwindy return gain_windydef read_data(): print(\"Nilai gain information dari outlook = \",rdoutlook()) print(\"Nilai gain information dari temperature = \",rdtemperature()) print(\"Nilai gain information dari humidity = \",rdhumidity()) print(\"Nilai gain information dari windy = \",rdwindy()) read_data() HASIL outlook temperature humidity windy play Unnamed: 5 Unnamed: 6 \\ 0 sunny hot high False no no yes 1 sunny hot high True no 5 9 2 overcast hot high False yes p1=5/14 p2=9/14 3 rainy mild high False yes NaN NaN 4 rainy cool normal False yes NaN NaN 5 rainy cool normal True no NaN NaN 6 overcast cool normal True yes NaN NaN 7 sunny mild high False no NaN NaN 8 sunny cool normal False yes NaN NaN 9 rainy mild normal False yes NaN NaN 10 sunny mild normal True yes NaN NaN 11 overcast mild high True yes NaN NaN 12 overcast hot normal False yes NaN NaN 13 rainy mild high True no NaN NaN Unnamed: 7 Unnamed: 8 0 NaN NaN 1 NaN 0.940286 2 NaN NaN 3 NaN NaN 4 NaN NaN 5 NaN NaN 6 NaN NaN 7 NaN NaN 8 NaN NaN 9 NaN NaN 10 NaN NaN 11 NaN NaN 12 NaN NaN 13 NaN NaN Nilai gain information dari outlook = 0.24674981977443933 Nilai gain information dari temperature = 0.02922256565895487 Nilai gain information dari humidity = 0.15183550136234159 Nilai gain information dari windy = 0.0481270304082694","title":"Home"},{"location":"#tugas-2","text":"Menghitung jarak pada data Pengertian, Rumus , dan Penjelasan Menghitung Jarak Pada Data","title":"Tugas 2"},{"location":"#a-cara-kerja-algoritma-k-nearest-neighbors-knn","text":"K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak.","title":"A. Cara Kerja Algoritma K-Nearest Neighbors (KNN)"},{"location":"#klasifikasi-terdekat-nearest-neighbor-classification","text":"Data baru yang diklasifikasi selanjutnya diproyeksikan pada ruang dimensi banyak yang telah memuat titik-titik c data pembelajaran. Proses klasifikasi dilakukan dengan mencari titik c terdekat dari c-baru ( nearest neighbor ) . Teknik pencarian tetangga terdekat yang umum dilakukan dengan menggunakan formula jarak euclidean . Berikut beberapa formula yang digunakan dalam algoritma knn.","title":"Klasifikasi Terdekat (Nearest Neighbor Classification)"},{"location":"#euclidean-distance","text":"Jarak Euclidean adalah formula untuk mencari jarak antara 2 titik dalam ruang dua dimensi.","title":"Euclidean Distance"},{"location":"#hamming-distance","text":"Jarak Hamming adalah cara mencari jarak antar 2 titik yang dihitung dengan panjang vektor biner yang dibentuk oleh dua titik tersebut dalam block kode biner.","title":"Hamming Distance"},{"location":"#manhattan-distance","text":"Manhattan Distance atau Taxicab Geometri adalah formula untuk mencari jarak d antar 2 vektor p,q pada ruang dimensi n .","title":"Manhattan Distance"},{"location":"#minkowski-distance","text":"Minkowski distance adalah formula pengukuran antar 2 titik pada ruang vektor normal yang merupakan hibridisasi yang mengeneralisasi euclidean distance dan mahattan distance. Diberikan dua obyek yang diwakili oleh tuple (a,b,c,d,e,f) dan (f,g,h,i,j). Kita misalkan obyek pertama dengan i = (a,b,c,d,e) dan obyek kedua dengan j = (f,g,h,i,j) berikut nilai-nilainya : i j a = 41 91=3731 f = 41 80=3280 b = 41 92=3772 g = 41 81=3321 c = 41 93=3813 h = 41 82=3362 d = 41 94=3854 i = 41 83=3403 e = 41 95=3895 j = 41 84=3444 memanggil fungsi yang aa pada library scipy dan membaca file exel yang telah dibuat. from scipy import stats import pandas as pd pd.read_csv(\"Tugas2.csv\") Hasil Run 14 8 440 215 4312 8.5 70 1 plymouth fury iii 0 14.0 8 455.0 225 4425 10.0 70 1 pontiac catalina 1 15.0 8 390.0 190 3850 8.5 70 1 amc ambassador dpl 2 15.0 8 383.0 170 3563 10.0 70 1 dodge challenger se 3 14.0 8 340.0 160 3609 8.0 70 1 plymouth 'cuda 340 4 15.0 8 400.0 150 3761 9.5 70 1 chevrolet monte carlo ... ... ... ... ... ... ... ... ... ... 385 27.0 4 140.0 86 2790 15.6 82 1 ford mustang gl 386 44.0 4 97.0 52 2130 24.6 82 2 vw pickup 387 32.0 4 135.0 84 2295 11.6 82 1 dodge rampage 388 28.0 4 120.0 79 2625 18.6 82 1 ford ranger 389 31.0 4 119.0 82 2720 19.4 82 1 chevy s-10 390 rows \u00d7 9 columns Mencari jarak dengan methode Euclidia Distance dan methode manahattanDistance from scipy import stats import pandas as pd import math df=pd.read_csv(\"servo.csv\") def Euclidean(i,j): hasil = 0 for ii in range(len(df)-1): rumus = (i[ii]-j[ii])**2 hasil = hasil + rumus return math.sqrt(hasil) print(\"M = 2\") print(\"jarak kedua objek adalah :\",Euclidean(df['vgain'],df['class'])) def Manahattan(i,j): hasil = 0 for kk in range(len(df)-1): rumus = i[kk]-j[kk] hasil = hasil + rumus return hasil print(\"M = 1\") print(\"jarak kedua objek adalah :\",Manahattan(df['vgain'],df['class'])) HASIL RUN M = 2 jarak kedua objek adalah : 34.30324786357754 M = 1 jarak kedua objek adalah : 187.72495382000025","title":"Minkowski Distance"},{"location":"#tugas-3","text":"","title":"TUGAS 3"},{"location":"#feature-selection","text":"","title":"feature selection"},{"location":"#entropy-menggunakan-tabel-frekuensi-dari-satu-atribut","text":"","title":"Entropy menggunakan tabel frekuensi dari satu atribut"},{"location":"#information-gain","text":"Ada banyak teknik untuk menyeleksi fitur. Information Gain bekerja dengan mendeteksi fitur yang paling banyak memiliki informasi berdasarkan kelas tertentu dengan menghitung nilai Entropy-nya Rumus untuk mencari Entropy Target adalah: Entropy(S): c\u2211i\u2212Pilog2PiEntropy(S): \u2211ic\u2212Pilog2\u2061Pi Dimana c adalah jumlah nilai yang ada pada kelas kategori dan Pi adalah jumlah sampel untuk kelas i import numpy as np import seaborn as sea import matplotlib.pyplot as mat from scipy import stats import pandas as data import math as rumus sample = data.read_csv(\"feture selection.csv\") print(sample) def entropy(b,a): result = -(a*rumus.log2(a))-(b*rumus.log2(b)) return result def play(yes,no): dataclass = [] for i in sample['play']: dataclass.append(i) data = len(dataclass) tempyes = yes/data tempno = no/data result = entropy(tempyes,tempno) return result mother_value=play(9,5)","title":"Information Gain"},{"location":"#entropy-menggunakan-tabel-frekuensi-dengan-dua-attribut","text":"def Entropy(b,a,c): result = -(a/c*rumus.log2(a/c))-(b/c*rumus.log2(b/c)) return result","title":"Entropy menggunakan tabel frekuensi dengan dua attribut"},{"location":"#untuk-outlook","text":"def outlook(sunny_yes,sunny_no,overcast_yes,overcast_no,rainy_yes,rainy_no): data_sunny = sunny_yes + sunny_no data_overcast = overcast_yes +overcast_no data_rainy = rainy_yes + rainy_no data = data_sunny + data_overcast + data_rainy E_sunny = Entropy(sunny_no,sunny_yes,data_sunny) E_overcast = 0 E_rainy = Entropy(rainy_no,rainy_yes,data_rainy) count = (data_sunny/data)*E_sunny + (data_overcast/data) *0 + (data_rainy/data)*E_rainy return count","title":"untuk outlook"},{"location":"#untuk-temperature","text":"def temperature(hot_yes,hot_no,mild_yes,mild_no,cool_yes,cool_no): data_hot = hot_yes + hot_no data_mild = mild_yes +mild_no data_cool = cool_yes + cool_no data = data_hot + data_mild + data_cool E_hot = Entropy(hot_no,hot_yes,data_hot) E_mild = Entropy(mild_no,mild_yes,data_mild) E_cool = Entropy(cool_no,cool_yes,data_cool) count = (data_hot/data)*E_hot + (data_mild/data) *E_mild + (data_cool/data)*E_cool return count","title":"untuk temperature"},{"location":"#untuk-humidity","text":"def Humidity(high_yes,high_no,normal_yes,normal_no): data_high = high_yes + high_no data_normal = normal_yes +normal_no data = data_high + data_normal E_high = Entropy(high_yes,high_no,data_high) E_normal = Entropy(normal_yes,normal_no,data_normal) count = (data_high/data)*E_high + (data_normal/data) *E_normal return count","title":"untuk Humidity"},{"location":"#untuk-windy","text":"def windy(False_yes,False_no,True_yes,True_no): data_False = False_yes + False_no data_True = True_yes + True_no data = data_False + data_True E_false = Entropy(False_yes,False_no,data_False) E_true = Entropy(True_yes,True_no,data_True) count = (data_False/data)*E_false + (data_True/data) *E_true return count def rdoutlook(): count = 0 temp=[] rainy_yes = [] rainy_no = [] overcast_yes = [] overcast_no =[] sunny_yes = [] sunny_no =[] for v in sample['play']: if (v == \"yes\"): temp.append(\"yes\") else: temp.append(\"no\") for i in sample['outlook']: if (i == \"rainy\" and temp[count] == \"yes\"): rainy_yes.append(i) elif (i == \"rainy\" and temp[count] == \"no\"): rainy_no.append(i) elif (i == \"overcast\" and temp[count] == \"yes\"): overcast_yes.append(i) elif (i == \"overcast\" and temp[count] == \"no\"): overcast_no.append(i) elif (i == \"sunny\" and temp[count] == \"yes\"): sunny_yes.append(i) elif (i == \"sunny\" and temp[count] == \"no\"): sunny_no.append(i) count += 1 Voutlook=outlook(len(rainy_yes),len(rainy_no),len(overcast_yes),len(overcast_no),len(sunny_yes),len(sunny_no)) gain_outlook = mother_value - Voutlook return gain_outlook def rdtemperature(): count = 0 temp=[] hot_yes = [] hot_no = [] mild_yes = [] mild_no =[] cool_yes = [] cool_no =[] for v in sample['play']: if (v == \"yes\"): temp.append(\"yes\") else: temp.append(\"no\") for i in sample['temperature']: if (i == \"hot\" and temp[count] == \"yes\"): hot_yes.append(i) elif (i == \"hot\" and temp[count] == \"no\"): hot_no.append(i) elif (i == \"mild\" and temp[count] == \"yes\"): mild_yes.append(i) elif (i == \"mild\" and temp[count] == \"no\"): mild_no.append(i) elif (i == \"cool\" and temp[count] == \"yes\"): cool_yes.append(i) elif (i == \"cool\" and temp[count] == \"no\"): cool_no.append(i) count += 1 Vtemperature=temperature(len(hot_yes),len(hot_no),len(mild_yes),len(mild_no),len(cool_yes),len(cool_no)) gain_temperature = mother_value - Vtemperature return gain_temperature def rdhumidity(): count = 0 temp=[] high_yes = [] high_no = [] normal_yes = [] normal_no =[] for v in sample['play']: if (v == \"yes\"): temp.append(\"yes\") else: temp.append(\"no\") for i in sample['humidity']: if (i == \"high\" and temp[count] == \"yes\"): high_yes.append(i) elif (i == \"high\" and temp[count] == \"no\"): high_no.append(i) elif (i == \"normal\" and temp[count] == \"yes\"): normal_yes.append(i) elif (i == \"normal\" and temp[count] == \"no\"): normal_no.append(i) count += 1 Vhumidity=Humidity(len(high_yes),len(high_no),len(normal_yes),len(normal_no)) gain_humidity = mother_value - Vhumidity return gain_humiditydef rdwindy(): count = 0 temp=[] false_yes = [] false_no = [] true_yes = [] true_no =[] for v in sample['play']: if (v == \"yes\"): temp.append(\"yes\") else: temp.append(\"no\") for i in sample['windy']: if (i == False and temp[count] == \"yes\"): false_yes.append(i) elif (i == False and temp[count] == \"no\"): false_no.append(i) elif (i == True and temp[count] == \"yes\"): true_yes.append(i) elif (i == True and temp[count] == \"no\"): true_no.append(i) count += 1 Vwindy=windy(len(false_yes),len(false_no),len(true_yes),len(true_no)) gain_windy = mother_value - Vwindy return gain_windydef read_data(): print(\"Nilai gain information dari outlook = \",rdoutlook()) print(\"Nilai gain information dari temperature = \",rdtemperature()) print(\"Nilai gain information dari humidity = \",rdhumidity()) print(\"Nilai gain information dari windy = \",rdwindy()) read_data()","title":"untuk windy"},{"location":"#hasil","text":"outlook temperature humidity windy play Unnamed: 5 Unnamed: 6 \\ 0 sunny hot high False no no yes 1 sunny hot high True no 5 9 2 overcast hot high False yes p1=5/14 p2=9/14 3 rainy mild high False yes NaN NaN 4 rainy cool normal False yes NaN NaN 5 rainy cool normal True no NaN NaN 6 overcast cool normal True yes NaN NaN 7 sunny mild high False no NaN NaN 8 sunny cool normal False yes NaN NaN 9 rainy mild normal False yes NaN NaN 10 sunny mild normal True yes NaN NaN 11 overcast mild high True yes NaN NaN 12 overcast hot normal False yes NaN NaN 13 rainy mild high True no NaN NaN Unnamed: 7 Unnamed: 8 0 NaN NaN 1 NaN 0.940286 2 NaN NaN 3 NaN NaN 4 NaN NaN 5 NaN NaN 6 NaN NaN 7 NaN NaN 8 NaN NaN 9 NaN NaN 10 NaN NaN 11 NaN NaN 12 NaN NaN 13 NaN NaN Nilai gain information dari outlook = 0.24674981977443933 Nilai gain information dari temperature = 0.02922256565895487 Nilai gain information dari humidity = 0.15183550136234159 Nilai gain information dari windy = 0.0481270304082694","title":"HASIL"},{"location":"Fuzzy clustering/","text":"Fuzzy clustering \u200b Fuzzy clustering (juga disebut sebagai soft clustering atau soft k -means ) adalah bentuk pengelompokan di mana setiap titik data dapat menjadi milik lebih dari satu cluster. \u200b Clustering atau analisis cluster melibatkan penugasan poin data ke cluster sehingga item dalam cluster yang sama adalah sama mungkin, sementara item milik cluster berbeda sama mungkin. Cluster diidentifikasi melalui langkah-langkah kesamaan. Langkah-langkah kesamaan ini mencakup jarak, konektivitas, dan intensitas. \u200b Algoritma FCM (Fuzzy C-Means) Clustering adalah salah satu algoritma yang digunakan dalam pengolahan citra. . Algoritma ini merupakan penggabungan dari Algoritma Fuzzy Logic dan Algoritma K-Means Clustering. K-Means Clustering adalah salah satu algoritma klasifikasi data yang cukup banyak dipakai untuk memecahkan masalah. Hanya saja metode tersebut tidak memiliki nilai pengembalian berupa sebuah nilai pembanding untuk masing-masing cluster, sehingga digunakan algoritma Fuzzy untuk menghitung skor dari sebuah data. Contoh Penggunaan FCM pada data: import numpy as np import seaborn as sea import matplotlib.pyplot as mat from scipy import stats import pandas as calldata import math as rumus import statistics as s Data = calldata.read_csv(\"clustering.csv\") Data = Data[['STG','SCG','STR','LPR','PEG']].sample(6, random_state=42) D = Data.values print(\"Table (D) >>\") n, m, c, w, T, e, P0, t = *D.shape, 2, 2, 10, 0.1, 0, 1 print(\"Variables >>\") print(\" n = %d\\n m = %d\\n c = %d\\n w = %d\\n T = %d\\n e = %f\\n P0 = %d\\n t = %d\" % (n, m, c, w, T, e, P0, t)) random.seed(42) U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) print(\"U >>\\n\") print(U) def cluster(U, D, x, y): return sum([U[i,y]**w*D[i,x] for i in range(n)])/sum([U[i,y]**w for i in range(n)]) V = np.array([[cluster(U,D,x,y) for x in range(m)] for y in range(c)]) print(\"V >>\\n\") print(V) def objective(V,U,D): return sum([sum([sum([(D[i,j]-V[k,j])**2 for j in range(m)])*(U[i,k]**w) for k in range(c)]) for i in range(n)]) Pt = objective(V,U,D) print(\"Pt >>\\n\") print(Pt) def converge(V,D,i,k): return (sum([(D[i,j]-V[k,j])**2 for j in range(2)])**(-1/(w-1)))/sum([sum([(D[i,j]-V[k,j])**2 for j in range(2)])**(-1/(w-1)) for k in range(2)]) print(\"U >>\\n\") #np.array([[converge(V,D,i,k) for k in range(2)] for i in range(15)]) def iterate(U): V = np.array([[cluster(U, D, x, y) for x in range(m)] for y in range(c)]) return np.array([[converge(V,D,i,k) for k in range(c)] for i in range(n)]), objective(V,U,D) def fuzzyCM(U): U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) U, P2, P, t = *iterate(U), 0, 1 while abs(P2 - P) > e and t < T: U, P2, P, t = *iterate(U), P2, t+1 return U, t FuzzyResult, FuzzyIters = fuzzyCM(U) print(\"Iterating %d times, fuzzy result >> \\n\" % FuzzyIters) print(FuzzyResult) table(calldata([D[i].tolist()+[np.argmax(FuzzyResult[i].tolist())] for i in range(15)], columns=Data.columns.tolist()+[\"Cluster Index\"])) Hasil Run: Table (D) >> Variables >> n = 6 m = 5 c = 2 w = 2 T = 10 e = 0.100000 P0 = 0 t = 1 U >> [[0.6394268 0.02501076] [0.27502932 0.22321074] [0.73647121 0.67669949] [0.89217957 0.08693883] [0.42192182 0.02979722] [0.21863797 0.50535529]] V >> [[0.10251419 0.19701892 0.44578176 0.47658428 0.43585526] [0.06380862 0.06682769 0.52281315 0.53336766 0.49281518]] Pt >> 0.37014293691958705 U >> Iterating 2 times, fuzzy result >> [[0.8593773 0.1406227 ] [0.11596106 0.88403894] [0.02027774 0.97972226] [0.82802879 0.17197121] [0.51110547 0.48889453] [0.27517007 0.72482993]]","title":"Fuzzy clustering"},{"location":"Fuzzy clustering/#fuzzy-clustering","text":"\u200b Fuzzy clustering (juga disebut sebagai soft clustering atau soft k -means ) adalah bentuk pengelompokan di mana setiap titik data dapat menjadi milik lebih dari satu cluster. \u200b Clustering atau analisis cluster melibatkan penugasan poin data ke cluster sehingga item dalam cluster yang sama adalah sama mungkin, sementara item milik cluster berbeda sama mungkin. Cluster diidentifikasi melalui langkah-langkah kesamaan. Langkah-langkah kesamaan ini mencakup jarak, konektivitas, dan intensitas. \u200b Algoritma FCM (Fuzzy C-Means) Clustering adalah salah satu algoritma yang digunakan dalam pengolahan citra. . Algoritma ini merupakan penggabungan dari Algoritma Fuzzy Logic dan Algoritma K-Means Clustering. K-Means Clustering adalah salah satu algoritma klasifikasi data yang cukup banyak dipakai untuk memecahkan masalah. Hanya saja metode tersebut tidak memiliki nilai pengembalian berupa sebuah nilai pembanding untuk masing-masing cluster, sehingga digunakan algoritma Fuzzy untuk menghitung skor dari sebuah data. Contoh Penggunaan FCM pada data: import numpy as np import seaborn as sea import matplotlib.pyplot as mat from scipy import stats import pandas as calldata import math as rumus import statistics as s Data = calldata.read_csv(\"clustering.csv\") Data = Data[['STG','SCG','STR','LPR','PEG']].sample(6, random_state=42) D = Data.values print(\"Table (D) >>\") n, m, c, w, T, e, P0, t = *D.shape, 2, 2, 10, 0.1, 0, 1 print(\"Variables >>\") print(\" n = %d\\n m = %d\\n c = %d\\n w = %d\\n T = %d\\n e = %f\\n P0 = %d\\n t = %d\" % (n, m, c, w, T, e, P0, t)) random.seed(42) U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) print(\"U >>\\n\") print(U) def cluster(U, D, x, y): return sum([U[i,y]**w*D[i,x] for i in range(n)])/sum([U[i,y]**w for i in range(n)]) V = np.array([[cluster(U,D,x,y) for x in range(m)] for y in range(c)]) print(\"V >>\\n\") print(V) def objective(V,U,D): return sum([sum([sum([(D[i,j]-V[k,j])**2 for j in range(m)])*(U[i,k]**w) for k in range(c)]) for i in range(n)]) Pt = objective(V,U,D) print(\"Pt >>\\n\") print(Pt) def converge(V,D,i,k): return (sum([(D[i,j]-V[k,j])**2 for j in range(2)])**(-1/(w-1)))/sum([sum([(D[i,j]-V[k,j])**2 for j in range(2)])**(-1/(w-1)) for k in range(2)]) print(\"U >>\\n\") #np.array([[converge(V,D,i,k) for k in range(2)] for i in range(15)]) def iterate(U): V = np.array([[cluster(U, D, x, y) for x in range(m)] for y in range(c)]) return np.array([[converge(V,D,i,k) for k in range(c)] for i in range(n)]), objective(V,U,D) def fuzzyCM(U): U = np.array([[random.uniform(0, 1) for _ in range(c)] for _ in range(n)]) U, P2, P, t = *iterate(U), 0, 1 while abs(P2 - P) > e and t < T: U, P2, P, t = *iterate(U), P2, t+1 return U, t FuzzyResult, FuzzyIters = fuzzyCM(U) print(\"Iterating %d times, fuzzy result >> \\n\" % FuzzyIters) print(FuzzyResult) table(calldata([D[i].tolist()+[np.argmax(FuzzyResult[i].tolist())] for i in range(15)], columns=Data.columns.tolist()+[\"Cluster Index\"]))","title":"Fuzzy clustering"},{"location":"Fuzzy clustering/#hasil-run","text":"Table (D) >> Variables >> n = 6 m = 5 c = 2 w = 2 T = 10 e = 0.100000 P0 = 0 t = 1 U >> [[0.6394268 0.02501076] [0.27502932 0.22321074] [0.73647121 0.67669949] [0.89217957 0.08693883] [0.42192182 0.02979722] [0.21863797 0.50535529]] V >> [[0.10251419 0.19701892 0.44578176 0.47658428 0.43585526] [0.06380862 0.06682769 0.52281315 0.53336766 0.49281518]] Pt >> 0.37014293691958705 U >> Iterating 2 times, fuzzy result >> [[0.8593773 0.1406227 ] [0.11596106 0.88403894] [0.02027774 0.97972226] [0.82802879 0.17197121] [0.51110547 0.48889453] [0.27517007 0.72482993]]","title":"Hasil Run:"},{"location":"TUGAS 4/","text":"TUGAS 4 Naive Bayes menggunakan data iris bunga Naive Bayes Classifier \u200b Algoritma Naive Bayes merupakan sebuah metoda klasifikasi menggunakan metode probabilitas dan statistik yg dikemukakan oleh ilmuwan Inggris Thomas Bayes. Algoritma Naive Bayes memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya sehingga dikenal sebagai Teorema Bayes. Ciri utama dr Na\u00efve Bayes Classifier ini adalah asumsi yg sangat kuat (na\u00eff) akan independensi dari masing-masing kondisi / kejadian. import numpy as np import seaborn as sea import matplotlib.pyplot as mat from scipy import stats import pandas as calldata import math as rumus import statistics as s data = calldata.read_csv(\"datairisbunga.csv\") print(data,\"\\n == Likelihood\") data_Iris = [] data_Iris_setosa=[] data_Iris_versicolor=[] data_Iris_virginica=[] for i in data['class']: data_Iris.append(i) if i == 'Iris-setosa': data_Iris_setosa.append(i) elif i == 'Iris-versicolor': data_Iris_versicolor.append(i) elif i == 'Iris-virginica': data_Iris_virginica.append(i) jumlah_data_Iris = len(data_Iris) jumlah_Iris_setosa = len(data_Iris_setosa) jumlah_Iris_versicolor = len(data_Iris_versicolor) jumlah_Iris_virginica = len(data_Iris_virginica) def sepallength_likelihood(i1,i2,i3,alldata): data4 = [] data5 = [] data6 = [] data7 = [] for i in data['sepallength']: if i >= 4.0 and i <= 4.9: data4.append(i) elif i >=5.0 and i <=5.9: data5.append(i) elif i >=6.0 and i <=6.9: data6.append(i) elif i >=7.0 and i <=7.9: data7.append(i) #iris setosa #P(c|x) = P(iris setosa| jangkauan 4 ) Pxc_data4_is = (len(data4)/i1)*(i1/alldata)/(len(data4)/alldata) #P(c|x) = P(iris setosa| jangkauan 5 ) Pxc_data5_is= (len(data5)/i1)*(i1/alldata)/(len(data5)/alldata) #P(c|x) = P(iris setosa| jangkauan 6 ) Pxc_data6_is= (len(data6)/i1)*(i1/alldata)/(len(data6)/alldata) #P(c|x) = P(iris setosa| jangkauan 7 ) Pxc_data7_is= (len(data7)/i1)*(i1/alldata)/(len(data7)/alldata) Iris_setosa = Pxc_data4_is*Pxc_data5_is*Pxc_data6_is*Pxc_data7_is #iris versicolor #P(c|x) = P(iris versicolor| jangkauan 4 ) Pxc_data4_iver= (len(data4)/i2)*(i2/alldata)/(len(data4)/alldata) #P(c|x) = P(iris versicolor| jangkauan 5 ) Pxc_data5_iver= (len(data5)/i2)*(i2/alldata)/(len(data5)/alldata) #P(c|x) = P(iris versicolor| jangkauan 6 ) Pxc_data6_iver= (len(data6)/i2)*(i2/alldata)/(len(data6)/alldata) #P(c|x) = P(iris versicolor| jangkauan 7 ) Pxc_data7_iver= (len(data7)/i2)*(i2/alldata)/(len(data7)/alldata) Iris_versicolor = Pxc_data4_iver*Pxc_data5_iver*Pxc_data6_iver*Pxc_data7_iver #iris virginica #P(c|x) = P(iris virginica| jangkauan 4 ) Pxc_data4_ivir= (len(data4)/i3)*(i3/alldata)/(len(data4)/alldata) #P(c|x) = P(iris virginica| jangkauan 5 ) Pxc_data5_ivir= (len(data5)/i3)*(i3/alldata)/(len(data5)/alldata) #P(c|x) = P(iris virginica| jangkauan 6 ) Pxc_data6_ivir= (len(data6)/i3)*(i3/alldata)/(len(data6)/alldata) #P(c|x) = P(iris virginica| jangkauan 7 ) Pxc_data7_ivir= (len(data7)/i3)*(i3/alldata)/(len(data7)/alldata) Iris_virginica = Pxc_data4_ivir*Pxc_data5_ivir*Pxc_data6_ivir*Pxc_data7_ivir print (\"Sepallenght fitur \\nIris setosa = \",Iris_setosa,\"\\nIris versicolor = \",Iris_versicolor,\"\\nIris virginica = \",Iris_virginica) \u200b def sepalwidth_likelihood(i1,i2,i3,alldata): data2 = [] data3 = [] for i in data['sepalwidth']: if i >= 2.0 and i <= 2.9: data2.append(i) elif i >=3.0 and i <=3.9: data3.append(i) #iris setosa #P(c|x) = P(iris setosa| jangkauan 2 ) Pxc_data2_is = (len(data2)/i1)*(i1/alldata)/(len(data2)/alldata) #P(c|x) = P(iris setosa| jangkauan 3 ) Pxc_data3_is= (len(data3)/i1)*(i1/alldata)/(len(data3)/alldata) Iris_setosa = Pxc_data2_is*Pxc_data3_is #iris versicolor #P(c|x) = P(iris versicolor| jangkauan 2 ) Pxc_data2_iver= (len(data2)/i2)*(i2/alldata)/(len(data2)/alldata) #P(c|x) = P(iris versicolor| jangkauan 3 ) Pxc_data3_iver= (len(data3)/i2)*(i2/alldata)/(len(data3)/alldata) Iris_versicolor = Pxc_data2_iver*Pxc_data3_iver #iris virginica #P(c|x) = P(iris virginica| jangkauan 2 ) Pxc_data2_ivir= (len(data2)/i3)*(i3/alldata)/(len(data3)/alldata) #P(c|x) = P(iris virginica| jangkauan 3 ) Pxc_data3_ivir= (len(data3)/i3)*(i3/alldata)/(len(data3)/alldata) Iris_virginica = Pxc_data2_ivir*Pxc_data3_ivir print (\"Sepalwidht fitur \\nIris setosa = \",Iris_setosa,\"\\nIris versicolor = \",Iris_versicolor,\"\\nIris virginica = \",Iris_virginica) def petallength_likelihood(i1,i2,i3,alldata): data2 = [] data3 = [] data4 = [] data5 = [] data6 = [] for i in data['petallength']: if i >= 1.0 and i <= 1.9: data2.append(i) elif i >=3.0 and i <=3.9: data3.append(i) elif i >=4.0 and i <=4.9: data4.append(i) elif i >=5.0 and i <=5.9: data5.append(i) elif i >=6.0 and i <=6.9: data6.append(i) #iris setosa #P(c|x) = P(iris setosa| jangkauan 2 ) Pxc_data2_is = (len(data2)/i1)*(i1/alldata)/(len(data2)/alldata) #P(c|x) = P(iris setosa| jangkauan 3 ) Pxc_data3_is= (len(data3)/i1)*(i1/alldata)/(len(data3)/alldata) #P(c|x) = P(iris setosa| jangkauan 4 ) Pxc_data4_is = (len(data4)/i1)*(i1/alldata)/(len(data4)/alldata) #P(c|x) = P(iris setosa| jangkauan 5 ) Pxc_data5_is= (len(data5)/i1)*(i1/alldata)/(len(data5)/alldata) #P(c|x) = P(iris setosa| jangkauan 6 ) Pxc_data6_is= (len(data6)/i1)*(i1/alldata)/(len(data6)/alldata) Iris_setosa = Pxc_data2_is*Pxc_data3_is*Pxc_data4_is*Pxc_data5_is*Pxc_data6_is #iris versicolor #P(c|x) = P(iris versicolor| jangkauan 2 ) Pxc_data2_iver = (len(data2)/i2)*(i2/alldata)/(len(data2)/alldata) #P(c|x) = P(iris versicolor| jangkauan 3 ) Pxc_data3_iver= (len(data3)/i2)*(i2/alldata)/(len(data3)/alldata) #P(c|x) = P(iris versicolor| jangkauan 4 ) Pxc_data4_iver = (len(data4)/i2)*(i2/alldata)/(len(data4)/alldata) #P(c|x) = P(iris versicolor| jangkauan 5 ) Pxc_data5_iver= (len(data5)/i2)*(i2/alldata)/(len(data5)/alldata) #P(c|x) = P(iris versicolor jangkauan 6 ) Pxc_data6_iver= (len(data6)/i2)*(i2/alldata)/(len(data6)/alldata) Iris_versicolor = Pxc_data2_iver*Pxc_data3_iver*Pxc_data4_iver*Pxc_data5_iver*Pxc_data6_iver #iris virginica #P(c|x) = P(iris virginica| jangkauan 2 ) Pxc_data2_ivir = (len(data2)/i3)*(i3/alldata)/(len(data2)/alldata) #P(c|x) = P(iris versicolor| jangkauan 3 ) Pxc_data3_ivir= (len(data3)/i3)*(i3/alldata)/(len(data3)/alldata) #P(c|x) = P(iris versicolor| jangkauan 4 ) Pxc_data4_ivir = (len(data4)/i3)*(i3/alldata)/(len(data4)/alldata) #P(c|x) = P(iris versicolor| jangkauan 5 ) Pxc_data5_ivir= (len(data5)/i3)*(i3/alldata)/(len(data5)/alldata) #P(c|x) = P(iris versicolor jangkauan 6 ) Pxc_data6_ivir= (len(data6)/i3)*(i3/alldata)/(len(data6)/alldata) Iris_virginica = Pxc_data2_ivir*Pxc_data3_ivir*Pxc_data4_ivir*Pxc_data5_ivir*Pxc_data6_ivir print (\"Petallength fitur \\nIris setosa = \",Iris_setosa,\"\\nIris versicolor = \",Iris_versicolor,\"\\nIris virginica = \",Iris_virginica) \u200b def petalwidth_likelihood(i1,i2,i3,alldata): data0 = [] data1 = [] data2 = [] for i in data['petalwidth']: if i >= 0.0 and i <= 0.9: data0.append(i) elif i >=1.0 and i <=1.9: data1.append(i) elif i >=2.0 and i <=2.9: data2.append(i) #iris setosa #P(c|x) = P(iris setosa| jangkauan 0 ) Pxc_data0_is = (len(data0)/i1)*(i1/alldata)/(len(data0)/alldata) #P(c|x) = P(iris setosa| jangkauan 1 ) Pxc_data1_is= (len(data1)/i1)*(i1/alldata)/(len(data1)/alldata) #P(c|x) = P(iris setosa| jangkauan 2 ) Pxc_data2_is= (len(data2)/i1)*(i1/alldata)/(len(data2)/alldata) Iris_setosa = Pxc_data0_is*Pxc_data1_is*Pxc_data2_is #iris versicolor #P(c|x) = P(iris versicolor| jangkauan 0 ) Pxc_data0_iver = (len(data0)/i2)*(i2/alldata)/(len(data0)/alldata) #P(c|x) = P(iris versicolor| jangkauan 1 ) Pxc_data1_iver= (len(data1)/i2)*(i2/alldata)/(len(data1)/alldata) #P(c|x) = P(iris versicolor| jangkauan 2 ) Pxc_data2_iver= (len(data2)/i2)*(i2/alldata)/(len(data2)/alldata) Iris_versicolor = Pxc_data0_iver*Pxc_data1_iver*Pxc_data2_iver #iris virginica #P(c|x) = P(iris virginica| jangkauan 0 ) Pxc_data0_ivir= (len(data0)/i3)*(i3/alldata)/(len(data0)/alldata) #P(c|x) = P(iris virginica| jangkauan 1 ) Pxc_data1_ivir= (len(data1)/i3)*(i3/alldata)/(len(data1)/alldata) #P(c|x) = P(iris virginica| jangkauan 2 ) Pxc_data2_ivir= (len(data2)/i3)*(i3/alldata)/(len(data2)/alldata) Iris_virginica = Pxc_data0_ivir*Pxc_data1_ivir*Pxc_data2_ivir print (\"Sepalwidht fitur \\nIris setosa = \",Iris_setosa,\"\\nIris versicolor = \",Iris_versicolor,\"\\nIris virginica = \",Iris_virginica) print(\" Numerical Predictor \\n:\") \u200b def sepallength_num(i1,i2,i3,alldata): data_class=[] data_sepallength=[] sepallength_is=[] sepallength_ver=[] sepallength_vir=[] for c in data['class']: data_class.append(c) for i in data['sepallength']: data_sepallength.append(i) for j in range(len(data_class)): if data_class [j] == 'Iris-setosa': sepallength_is.append(data_sepallength[j]) if data_class [j] == 'Iris-versicolor': sepallength_ver.append(data_sepallength[j]) if data_class [j] == 'Iris-virginica': sepallength_vir.append(data_sepallength[j]) vir_stdev = s.stdev(sepallength_vir) normal_dis_is = (1/(rumus.sqrt((2*rumus.pi))*s.stdev(sepallength_is)))-(rumus.pow((7.6-(s.mean(sepallength_is))),2)/(2*pow(s.stdev(sepallength_is),2))) print(\"P(sepallenght = 7.6|class = Iris-setosa) = \",normal_dis_is) normal_dis_ver = (1/(rumus.sqrt((2*rumus.pi))*s.stdev(sepallength_ver)))-(rumus.pow((7.6-(s.mean(sepallength_ver))),2)/(2*pow(s.stdev(sepallength_ver),2))) print(\"P(sepallenght = 7.6|class = Iris-versicolor) = \",normal_dis_ver) normal_dis_vir = (1/(rumus.sqrt((2*rumus.pi))*s.stdev(sepallength_vir)))-(rumus.pow((7.6-(s.mean(sepallength_vir))),2)/(2*pow(s.stdev(sepallength_vir),2))) print(\"P(sepallenght = 7.6|class = Iris-virginica) = \",normal_dis_vir) sepallength_num(jumlah_Iris_setosa,jumlah_Iris_versicolor,jumlah_Iris_virginica,jumlah_data_Iris) OUTPUT sepallength sepalwidth petallength petalwidth class \\ 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 5 5.4 3.9 1.7 0.4 Iris-setosa 6 4.6 3.4 1.4 0.3 Iris-setosa 7 5.0 3.4 1.5 0.2 Iris-setosa 8 4.4 2.9 1.4 0.2 Iris-setosa 9 4.9 3.1 1.5 0.1 Iris-setosa 10 7.0 3.2 4.7 1.4 Iris-versicolor 11 6.4 3.2 4.5 1.5 Iris-versicolor 12 6.9 3.1 4.9 1.5 Iris-versicolor 13 5.5 2.3 4.0 1.3 Iris-versicolor 14 6.5 2.8 4.6 1.5 Iris-versicolor 15 5.7 2.8 4.5 1.3 Iris-versicolor 16 6.3 3.3 4.7 1.6 Iris-versicolor 17 4.9 2.4 3.3 1.0 Iris-versicolor 18 6.6 2.9 4.6 1.3 Iris-versicolor 19 5.2 2.7 3.9 1.4 Iris-versicolor 20 6.3 3.3 6.0 2.5 Iris-virginica 21 5.8 2.7 5.1 1.9 Iris-virginica 22 7.1 3.0 5.9 2.1 Iris-virginica 23 6.3 2.9 5.6 1.8 Iris-virginica 24 6.5 3.0 5.8 2.2 Iris-virginica 25 7.6 3.0 6.6 2.1 Iris-virginica 26 4.9 2.5 4.5 1.7 Iris-virginica 27 7.3 2.9 6.3 1.8 Iris-virginica 28 6.7 2.5 5.8 1.8 Iris-virginica 29 7.2 3.6 6.1 2.5 Iris-virginica Class Prediksi 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN 5 NaN 6 NaN 7 NaN 8 NaN 9 NaN 10 NaN 11 NaN 12 NaN 13 NaN 14 NaN 15 NaN 16 NaN 17 NaN 18 NaN 19 NaN 20 NaN 21 NaN 22 NaN 23 NaN 24 NaN 25 NaN 26 NaN 27 NaN 28 NaN 29 NaN == Likelihood Numerical Predictor : P(sepallenght = 7.6|class = Iris-setosa) = -42.85090110402189 P(sepallenght = 7.6|class = Iris-versicolor) = -1.5785361570686334 P(sepallenght = 7.6|class = Iris-virginica) = -0.32408451529143567","title":"TUGAS 4"},{"location":"TUGAS 4/#tugas-4","text":"","title":"TUGAS 4"},{"location":"TUGAS 4/#naive-bayes-menggunakan-data-iris-bunga","text":"Naive Bayes Classifier \u200b Algoritma Naive Bayes merupakan sebuah metoda klasifikasi menggunakan metode probabilitas dan statistik yg dikemukakan oleh ilmuwan Inggris Thomas Bayes. Algoritma Naive Bayes memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya sehingga dikenal sebagai Teorema Bayes. Ciri utama dr Na\u00efve Bayes Classifier ini adalah asumsi yg sangat kuat (na\u00eff) akan independensi dari masing-masing kondisi / kejadian. import numpy as np import seaborn as sea import matplotlib.pyplot as mat from scipy import stats import pandas as calldata import math as rumus import statistics as s data = calldata.read_csv(\"datairisbunga.csv\") print(data,\"\\n == Likelihood\") data_Iris = [] data_Iris_setosa=[] data_Iris_versicolor=[] data_Iris_virginica=[] for i in data['class']: data_Iris.append(i) if i == 'Iris-setosa': data_Iris_setosa.append(i) elif i == 'Iris-versicolor': data_Iris_versicolor.append(i) elif i == 'Iris-virginica': data_Iris_virginica.append(i) jumlah_data_Iris = len(data_Iris) jumlah_Iris_setosa = len(data_Iris_setosa) jumlah_Iris_versicolor = len(data_Iris_versicolor) jumlah_Iris_virginica = len(data_Iris_virginica) def sepallength_likelihood(i1,i2,i3,alldata): data4 = [] data5 = [] data6 = [] data7 = [] for i in data['sepallength']: if i >= 4.0 and i <= 4.9: data4.append(i) elif i >=5.0 and i <=5.9: data5.append(i) elif i >=6.0 and i <=6.9: data6.append(i) elif i >=7.0 and i <=7.9: data7.append(i) #iris setosa #P(c|x) = P(iris setosa| jangkauan 4 ) Pxc_data4_is = (len(data4)/i1)*(i1/alldata)/(len(data4)/alldata) #P(c|x) = P(iris setosa| jangkauan 5 ) Pxc_data5_is= (len(data5)/i1)*(i1/alldata)/(len(data5)/alldata) #P(c|x) = P(iris setosa| jangkauan 6 ) Pxc_data6_is= (len(data6)/i1)*(i1/alldata)/(len(data6)/alldata) #P(c|x) = P(iris setosa| jangkauan 7 ) Pxc_data7_is= (len(data7)/i1)*(i1/alldata)/(len(data7)/alldata) Iris_setosa = Pxc_data4_is*Pxc_data5_is*Pxc_data6_is*Pxc_data7_is #iris versicolor #P(c|x) = P(iris versicolor| jangkauan 4 ) Pxc_data4_iver= (len(data4)/i2)*(i2/alldata)/(len(data4)/alldata) #P(c|x) = P(iris versicolor| jangkauan 5 ) Pxc_data5_iver= (len(data5)/i2)*(i2/alldata)/(len(data5)/alldata) #P(c|x) = P(iris versicolor| jangkauan 6 ) Pxc_data6_iver= (len(data6)/i2)*(i2/alldata)/(len(data6)/alldata) #P(c|x) = P(iris versicolor| jangkauan 7 ) Pxc_data7_iver= (len(data7)/i2)*(i2/alldata)/(len(data7)/alldata) Iris_versicolor = Pxc_data4_iver*Pxc_data5_iver*Pxc_data6_iver*Pxc_data7_iver #iris virginica #P(c|x) = P(iris virginica| jangkauan 4 ) Pxc_data4_ivir= (len(data4)/i3)*(i3/alldata)/(len(data4)/alldata) #P(c|x) = P(iris virginica| jangkauan 5 ) Pxc_data5_ivir= (len(data5)/i3)*(i3/alldata)/(len(data5)/alldata) #P(c|x) = P(iris virginica| jangkauan 6 ) Pxc_data6_ivir= (len(data6)/i3)*(i3/alldata)/(len(data6)/alldata) #P(c|x) = P(iris virginica| jangkauan 7 ) Pxc_data7_ivir= (len(data7)/i3)*(i3/alldata)/(len(data7)/alldata) Iris_virginica = Pxc_data4_ivir*Pxc_data5_ivir*Pxc_data6_ivir*Pxc_data7_ivir print (\"Sepallenght fitur \\nIris setosa = \",Iris_setosa,\"\\nIris versicolor = \",Iris_versicolor,\"\\nIris virginica = \",Iris_virginica) \u200b def sepalwidth_likelihood(i1,i2,i3,alldata): data2 = [] data3 = [] for i in data['sepalwidth']: if i >= 2.0 and i <= 2.9: data2.append(i) elif i >=3.0 and i <=3.9: data3.append(i) #iris setosa #P(c|x) = P(iris setosa| jangkauan 2 ) Pxc_data2_is = (len(data2)/i1)*(i1/alldata)/(len(data2)/alldata) #P(c|x) = P(iris setosa| jangkauan 3 ) Pxc_data3_is= (len(data3)/i1)*(i1/alldata)/(len(data3)/alldata) Iris_setosa = Pxc_data2_is*Pxc_data3_is #iris versicolor #P(c|x) = P(iris versicolor| jangkauan 2 ) Pxc_data2_iver= (len(data2)/i2)*(i2/alldata)/(len(data2)/alldata) #P(c|x) = P(iris versicolor| jangkauan 3 ) Pxc_data3_iver= (len(data3)/i2)*(i2/alldata)/(len(data3)/alldata) Iris_versicolor = Pxc_data2_iver*Pxc_data3_iver #iris virginica #P(c|x) = P(iris virginica| jangkauan 2 ) Pxc_data2_ivir= (len(data2)/i3)*(i3/alldata)/(len(data3)/alldata) #P(c|x) = P(iris virginica| jangkauan 3 ) Pxc_data3_ivir= (len(data3)/i3)*(i3/alldata)/(len(data3)/alldata) Iris_virginica = Pxc_data2_ivir*Pxc_data3_ivir print (\"Sepalwidht fitur \\nIris setosa = \",Iris_setosa,\"\\nIris versicolor = \",Iris_versicolor,\"\\nIris virginica = \",Iris_virginica) def petallength_likelihood(i1,i2,i3,alldata): data2 = [] data3 = [] data4 = [] data5 = [] data6 = [] for i in data['petallength']: if i >= 1.0 and i <= 1.9: data2.append(i) elif i >=3.0 and i <=3.9: data3.append(i) elif i >=4.0 and i <=4.9: data4.append(i) elif i >=5.0 and i <=5.9: data5.append(i) elif i >=6.0 and i <=6.9: data6.append(i) #iris setosa #P(c|x) = P(iris setosa| jangkauan 2 ) Pxc_data2_is = (len(data2)/i1)*(i1/alldata)/(len(data2)/alldata) #P(c|x) = P(iris setosa| jangkauan 3 ) Pxc_data3_is= (len(data3)/i1)*(i1/alldata)/(len(data3)/alldata) #P(c|x) = P(iris setosa| jangkauan 4 ) Pxc_data4_is = (len(data4)/i1)*(i1/alldata)/(len(data4)/alldata) #P(c|x) = P(iris setosa| jangkauan 5 ) Pxc_data5_is= (len(data5)/i1)*(i1/alldata)/(len(data5)/alldata) #P(c|x) = P(iris setosa| jangkauan 6 ) Pxc_data6_is= (len(data6)/i1)*(i1/alldata)/(len(data6)/alldata) Iris_setosa = Pxc_data2_is*Pxc_data3_is*Pxc_data4_is*Pxc_data5_is*Pxc_data6_is #iris versicolor #P(c|x) = P(iris versicolor| jangkauan 2 ) Pxc_data2_iver = (len(data2)/i2)*(i2/alldata)/(len(data2)/alldata) #P(c|x) = P(iris versicolor| jangkauan 3 ) Pxc_data3_iver= (len(data3)/i2)*(i2/alldata)/(len(data3)/alldata) #P(c|x) = P(iris versicolor| jangkauan 4 ) Pxc_data4_iver = (len(data4)/i2)*(i2/alldata)/(len(data4)/alldata) #P(c|x) = P(iris versicolor| jangkauan 5 ) Pxc_data5_iver= (len(data5)/i2)*(i2/alldata)/(len(data5)/alldata) #P(c|x) = P(iris versicolor jangkauan 6 ) Pxc_data6_iver= (len(data6)/i2)*(i2/alldata)/(len(data6)/alldata) Iris_versicolor = Pxc_data2_iver*Pxc_data3_iver*Pxc_data4_iver*Pxc_data5_iver*Pxc_data6_iver #iris virginica #P(c|x) = P(iris virginica| jangkauan 2 ) Pxc_data2_ivir = (len(data2)/i3)*(i3/alldata)/(len(data2)/alldata) #P(c|x) = P(iris versicolor| jangkauan 3 ) Pxc_data3_ivir= (len(data3)/i3)*(i3/alldata)/(len(data3)/alldata) #P(c|x) = P(iris versicolor| jangkauan 4 ) Pxc_data4_ivir = (len(data4)/i3)*(i3/alldata)/(len(data4)/alldata) #P(c|x) = P(iris versicolor| jangkauan 5 ) Pxc_data5_ivir= (len(data5)/i3)*(i3/alldata)/(len(data5)/alldata) #P(c|x) = P(iris versicolor jangkauan 6 ) Pxc_data6_ivir= (len(data6)/i3)*(i3/alldata)/(len(data6)/alldata) Iris_virginica = Pxc_data2_ivir*Pxc_data3_ivir*Pxc_data4_ivir*Pxc_data5_ivir*Pxc_data6_ivir print (\"Petallength fitur \\nIris setosa = \",Iris_setosa,\"\\nIris versicolor = \",Iris_versicolor,\"\\nIris virginica = \",Iris_virginica) \u200b def petalwidth_likelihood(i1,i2,i3,alldata): data0 = [] data1 = [] data2 = [] for i in data['petalwidth']: if i >= 0.0 and i <= 0.9: data0.append(i) elif i >=1.0 and i <=1.9: data1.append(i) elif i >=2.0 and i <=2.9: data2.append(i) #iris setosa #P(c|x) = P(iris setosa| jangkauan 0 ) Pxc_data0_is = (len(data0)/i1)*(i1/alldata)/(len(data0)/alldata) #P(c|x) = P(iris setosa| jangkauan 1 ) Pxc_data1_is= (len(data1)/i1)*(i1/alldata)/(len(data1)/alldata) #P(c|x) = P(iris setosa| jangkauan 2 ) Pxc_data2_is= (len(data2)/i1)*(i1/alldata)/(len(data2)/alldata) Iris_setosa = Pxc_data0_is*Pxc_data1_is*Pxc_data2_is #iris versicolor #P(c|x) = P(iris versicolor| jangkauan 0 ) Pxc_data0_iver = (len(data0)/i2)*(i2/alldata)/(len(data0)/alldata) #P(c|x) = P(iris versicolor| jangkauan 1 ) Pxc_data1_iver= (len(data1)/i2)*(i2/alldata)/(len(data1)/alldata) #P(c|x) = P(iris versicolor| jangkauan 2 ) Pxc_data2_iver= (len(data2)/i2)*(i2/alldata)/(len(data2)/alldata) Iris_versicolor = Pxc_data0_iver*Pxc_data1_iver*Pxc_data2_iver #iris virginica #P(c|x) = P(iris virginica| jangkauan 0 ) Pxc_data0_ivir= (len(data0)/i3)*(i3/alldata)/(len(data0)/alldata) #P(c|x) = P(iris virginica| jangkauan 1 ) Pxc_data1_ivir= (len(data1)/i3)*(i3/alldata)/(len(data1)/alldata) #P(c|x) = P(iris virginica| jangkauan 2 ) Pxc_data2_ivir= (len(data2)/i3)*(i3/alldata)/(len(data2)/alldata) Iris_virginica = Pxc_data0_ivir*Pxc_data1_ivir*Pxc_data2_ivir print (\"Sepalwidht fitur \\nIris setosa = \",Iris_setosa,\"\\nIris versicolor = \",Iris_versicolor,\"\\nIris virginica = \",Iris_virginica) print(\" Numerical Predictor \\n:\") \u200b def sepallength_num(i1,i2,i3,alldata): data_class=[] data_sepallength=[] sepallength_is=[] sepallength_ver=[] sepallength_vir=[] for c in data['class']: data_class.append(c) for i in data['sepallength']: data_sepallength.append(i) for j in range(len(data_class)): if data_class [j] == 'Iris-setosa': sepallength_is.append(data_sepallength[j]) if data_class [j] == 'Iris-versicolor': sepallength_ver.append(data_sepallength[j]) if data_class [j] == 'Iris-virginica': sepallength_vir.append(data_sepallength[j]) vir_stdev = s.stdev(sepallength_vir) normal_dis_is = (1/(rumus.sqrt((2*rumus.pi))*s.stdev(sepallength_is)))-(rumus.pow((7.6-(s.mean(sepallength_is))),2)/(2*pow(s.stdev(sepallength_is),2))) print(\"P(sepallenght = 7.6|class = Iris-setosa) = \",normal_dis_is) normal_dis_ver = (1/(rumus.sqrt((2*rumus.pi))*s.stdev(sepallength_ver)))-(rumus.pow((7.6-(s.mean(sepallength_ver))),2)/(2*pow(s.stdev(sepallength_ver),2))) print(\"P(sepallenght = 7.6|class = Iris-versicolor) = \",normal_dis_ver) normal_dis_vir = (1/(rumus.sqrt((2*rumus.pi))*s.stdev(sepallength_vir)))-(rumus.pow((7.6-(s.mean(sepallength_vir))),2)/(2*pow(s.stdev(sepallength_vir),2))) print(\"P(sepallenght = 7.6|class = Iris-virginica) = \",normal_dis_vir) sepallength_num(jumlah_Iris_setosa,jumlah_Iris_versicolor,jumlah_Iris_virginica,jumlah_data_Iris) OUTPUT sepallength sepalwidth petallength petalwidth class \\ 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 5 5.4 3.9 1.7 0.4 Iris-setosa 6 4.6 3.4 1.4 0.3 Iris-setosa 7 5.0 3.4 1.5 0.2 Iris-setosa 8 4.4 2.9 1.4 0.2 Iris-setosa 9 4.9 3.1 1.5 0.1 Iris-setosa 10 7.0 3.2 4.7 1.4 Iris-versicolor 11 6.4 3.2 4.5 1.5 Iris-versicolor 12 6.9 3.1 4.9 1.5 Iris-versicolor 13 5.5 2.3 4.0 1.3 Iris-versicolor 14 6.5 2.8 4.6 1.5 Iris-versicolor 15 5.7 2.8 4.5 1.3 Iris-versicolor 16 6.3 3.3 4.7 1.6 Iris-versicolor 17 4.9 2.4 3.3 1.0 Iris-versicolor 18 6.6 2.9 4.6 1.3 Iris-versicolor 19 5.2 2.7 3.9 1.4 Iris-versicolor 20 6.3 3.3 6.0 2.5 Iris-virginica 21 5.8 2.7 5.1 1.9 Iris-virginica 22 7.1 3.0 5.9 2.1 Iris-virginica 23 6.3 2.9 5.6 1.8 Iris-virginica 24 6.5 3.0 5.8 2.2 Iris-virginica 25 7.6 3.0 6.6 2.1 Iris-virginica 26 4.9 2.5 4.5 1.7 Iris-virginica 27 7.3 2.9 6.3 1.8 Iris-virginica 28 6.7 2.5 5.8 1.8 Iris-virginica 29 7.2 3.6 6.1 2.5 Iris-virginica Class Prediksi 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN 5 NaN 6 NaN 7 NaN 8 NaN 9 NaN 10 NaN 11 NaN 12 NaN 13 NaN 14 NaN 15 NaN 16 NaN 17 NaN 18 NaN 19 NaN 20 NaN 21 NaN 22 NaN 23 NaN 24 NaN 25 NaN 26 NaN 27 NaN 28 NaN 29 NaN == Likelihood Numerical Predictor : P(sepallenght = 7.6|class = Iris-setosa) = -42.85090110402189 P(sepallenght = 7.6|class = Iris-versicolor) = -1.5785361570686334 P(sepallenght = 7.6|class = Iris-virginica) = -0.32408451529143567","title":"Naive Bayes menggunakan data iris bunga"},{"location":"Tugas 5  K-NN/","text":"K-NN Algoritma K-Nearest Neighbor (K-NN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN . Tahapan Langkah Algoritma K-NN Menentukan parameter k (jumlah tetangga paling dekat). Menghitung kuadrat jarak eucliden objek terhadap data training yang diberikan. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah) Mengumpulkan kategori Y (Klasifikasi nearest neighbor berdasarkan nilai k) Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat dipredisikan kategori objek. Contoh Kasus Perhitungan K-NN \u00b6 Terdapat beberapa data yang berasal dari survey questioner tentang klasifikasi kualitas kertas tissue apakah baik atau jelek, dengan objek training dibawah ini menggunakan dua attribute yaitu daya tahan terhadap asam dan kekuatan. contoh perhitungan k-nn Akan diproduksi kembali kertas tissue dengan attribute X1=7 dan X2=4 , tanpa harus mengeluarkan biaya untuk melakukan survey, maka dapat diklasifikasikan kertas tissue tersebut termasuk yang baik atau jelek. contoh perhitungan k-nn (2) contoh perhitungan k-nn (3) Dengan mengurutkan jarak terkecil, semisal diambil K=3, maka perbandingan nya adalah 2 (Baik) >1 (Jelek). Maka dapat disimpulkan kertas tissue dengan attribute X1=7 dan X2=4 masuk ke kelas Baik. Contoh kasus sepallength sepalwidth petallength petalwidth iris sepal length stat 5.1 3.5 1.4 0.2 Iris-setosa Count 150 4.9 3 1.4 0.2 Iris-setosa Minimum 4.3 4.7 3.2 1.3 0.2 Iris-setosa Maximum 7.9 4.6 3.1 1.5 0.2 Iris-setosa Mean 5.84 5 3.6 1.4 0.2 Iris-setosa Median 5.8 5.4 3.9 1.7 0.4 Iris-setosa Mode 5 4.6 3.4 1.4 0.3 Iris-setosa Quartile 1 5.1 5 3.4 1.5 0.2 Iris-setosa Range 3.6 4.4 2.9 1.4 0.2 Iris-setosa Variance 0.69 4.9 3.1 1.5 0.1 Iris-setosa Standard Deviation 0.83 5.4 3.7 1.5 0.2 Iris-setosa Coefficient of Variation 14.20% 4.8 3.4 1.6 0.2 Iris-setosa Skewness 0.31 4.8 3 1.4 0.1 Iris-setosa Kurtosis -0.55 4.3 3 1.1 0.1 Iris-setosa 5.8 4 1.2 0.2 Iris-setosa 5.7 4.4 1.5 0.4 Iris-setosa 5.4 3.9 1.3 0.4 Iris-setosa 5.1 3.5 1.4 0.3 Iris-setosa 5.7 3.8 1.7 0.3 Iris-setosa 5.1 3.8 1.5 0.3 Iris-setosa 5.4 3.4 1.7 0.2 Iris-setosa 5.1 3.7 1.5 0.4 Iris-setosa 4.6 3.6 1 0.2 Iris-setosa 5.1 3.3 1.7 0.5 Iris-setosa 4.8 3.4 1.9 0.2 Iris-setosa 5 3 1.6 0.2 Iris-setosa 5 3.4 1.6 0.4 Iris-setosa 5.2 3.5 1.5 0.2 Iris-setosa 5.2 3.4 1.4 0.2 Iris-setosa 4.7 3.2 1.6 0.2 Iris-setosa 4.8 3.1 1.6 0.2 Iris-setosa 5.4 3.4 1.5 0.4 Iris-setosa 5.2 4.1 1.5 0.1 Iris-setosa 5.5 4.2 1.4 0.2 Iris-setosa 4.9 3.1 1.5 0.1 Iris-setosa 5 3.2 1.2 0.2 Iris-setosa 5.5 3.5 1.3 0.2 Iris-setosa 4.9 3.1 1.5 0.1 Iris-setosa 4.4 3 1.3 0.2 Iris-setosa 5.1 3.4 1.5 0.2 Iris-setosa 5 3.5 1.3 0.3 Iris-setosa 4.5 2.3 1.3 0.3 Iris-setosa 4.4 3.2 1.3 0.2 Iris-setosa 5 3.5 1.6 0.6 Iris-setosa 5.1 3.8 1.9 0.4 Iris-setosa 4.8 3 1.4 0.3 Iris-setosa 5.1 3.8 1.6 0.2 Iris-setosa 4.6 3.2 1.4 0.2 Iris-setosa 5.3 3.7 1.5 0.2 Iris-setosa 5 3.3 1.4 0.2 Iris-setosa 7 3.2 4.7 1.4 Iris-versicolor 6.4 3.2 4.5 1.5 Iris-versicolor 6.9 3.1 4.9 1.5 Iris-versicolor 5.5 2.3 4 1.3 Iris-versicolor 6.5 2.8 4.6 1.5 Iris-versicolor 5.7 2.8 4.5 1.3 Iris-versicolor 6.3 3.3 4.7 1.6 Iris-versicolor 4.9 2.4 3.3 1 Iris-versicolor 6.6 2.9 4.6 1.3 Iris-versicolor 5.2 2.7 3.9 1.4 Iris-versicolor 5 2 3.5 1 Iris-versicolor 5.9 3 4.2 1.5 Iris-versicolor 6 2.2 4 1 Iris-versicolor 6.1 2.9 4.7 1.4 Iris-versicolor 5.6 2.9 3.6 1.3 Iris-versicolor 6.7 3.1 4.4 1.4 Iris-versicolor 5.6 3 4.5 1.5 Iris-versicolor 5.8 2.7 4.1 1 Iris-versicolor 6.2 2.2 4.5 1.5 Iris-versicolor 5.6 2.5 3.9 1.1 Iris-versicolor 5.9 3.2 4.8 1.8 Iris-versicolor 6.1 2.8 4 1.3 Iris-versicolor 6.3 2.5 4.9 1.5 Iris-versicolor 6.1 2.8 4.7 1.2 Iris-versicolor 6.4 2.9 4.3 1.3 Iris-versicolor 6.6 3 4.4 1.4 Iris-versicolor 6.8 2.8 4.8 1.4 Iris-versicolor 6.7 3 5 1.7 Iris-versicolor 6 2.9 4.5 1.5 Iris-versicolor 5.7 2.6 3.5 1 Iris-versicolor 5.5 2.4 3.8 1.1 Iris-versicolor 5.5 2.4 3.7 1 Iris-versicolor 5.8 2.7 3.9 1.2 Iris-versicolor 6 2.7 5.1 1.6 Iris-versicolor 5.4 3 4.5 1.5 Iris-versicolor 6 3.4 4.5 1.6 Iris-versicolor 6.7 3.1 4.7 1.5 Iris-versicolor 6.3 2.3 4.4 1.3 Iris-versicolor 5.6 3 4.1 1.3 Iris-versicolor 5.5 2.5 4 1.3 Iris-versicolor 5.5 2.6 4.4 1.2 Iris-versicolor 6.1 3 4.6 1.4 Iris-versicolor 5.8 2.6 4 1.2 Iris-versicolor 5 2.3 3.3 1 Iris-versicolor 5.6 2.7 4.2 1.3 Iris-versicolor 5.7 3 4.2 1.2 Iris-versicolor 5.7 2.9 4.2 1.3 Iris-versicolor 6.2 2.9 4.3 1.3 Iris-versicolor 5.1 2.5 3 1.1 Iris-versicolor 5.7 2.8 4.1 1.3 Iris-versicolor 6.3 3.3 6 2.5 Iris-virginica 5.8 2.7 5.1 1.9 Iris-virginica 7.1 3 5.9 2.1 Iris-virginica 6.3 2.9 5.6 1.8 Iris-virginica 6.5 3 5.8 2.2 Iris-virginica 7.6 3 6.6 2.1 Iris-virginica 4.9 2.5 4.5 1.7 Iris-virginica 7.3 2.9 6.3 1.8 Iris-virginica 6.7 2.5 5.8 1.8 Iris-virginica 7.2 3.6 6.1 2.5 Iris-virginica 6.5 3.2 5.1 2 Iris-virginica 6.4 2.7 5.3 1.9 Iris-virginica 6.8 3 5.5 2.1 Iris-virginica 5.7 2.5 5 2 Iris-virginica 5.8 2.8 5.1 2.4 Iris-virginica 6.4 3.2 5.3 2.3 Iris-virginica 6.5 3 5.5 1.8 Iris-virginica 7.7 3.8 6.7 2.2 Iris-virginica 7.7 2.6 6.9 2.3 Iris-virginica 6 2.2 5 1.5 Iris-virginica 6.9 3.2 5.7 2.3 Iris-virginica 5.6 2.8 4.9 2 Iris-virginica 7.7 2.8 6.7 2 Iris-virginica 6.3 2.7 4.9 1.8 Iris-virginica 6.7 3.3 5.7 2.1 Iris-virginica 7.2 3.2 6 1.8 Iris-virginica 6.2 2.8 4.8 1.8 Iris-virginica 6.1 3 4.9 1.8 Iris-virginica 6.4 2.8 5.6 2.1 Iris-virginica 7.2 3 5.8 1.6 Iris-virginica 7.4 2.8 6.1 1.9 Iris-virginica 7.9 3.8 6.4 2 Iris-virginica 6.4 2.8 5.6 2.2 Iris-virginica 6.3 2.8 5.1 1.5 Iris-virginica 6.1 2.6 5.6 1.4 Iris-virginica 7.7 3 6.1 2.3 Iris-virginica 6.3 3.4 5.6 2.4 Iris-virginica 6.4 3.1 5.5 1.8 Iris-virginica 6 3 4.8 1.8 Iris-virginica 6.9 3.1 5.4 2.1 Iris-virginica 6.7 3.1 5.6 2.4 Iris-virginica 6.9 3.1 5.1 2.3 Iris-virginica 5.8 2.7 5.1 1.9 Iris-virginica 6.8 3.2 5.9 2.3 Iris-virginica 6.7 3.3 5.7 2.5 Iris-virginica 6.7 3 5.2 2.3 Iris-virginica 6.3 2.5 5 1.9 Iris-virginica 6.5 3 5.2 2 Iris-virginica 6.2 3.4 5.4 2.3 Iris-virginica 5.9 3 5.1 1.8 Iris-virginica Berikut adalah Code dari KNN dari data yang telah saya buat #tugas k-nn import numpy as np import seaborn as sea import matplotlib.pyplot as mat from scipy import stats import pandas as calldata import math from math import sqrt import statistics as s # membaca data data = calldata.read_csv(\"Iris_tugas5.csv\") #print(data) #data uji # nilai k k = int(input(\"masukkan nilai k = \")) #termasuk class apa data diatas ? # array untuk menampung data dari data asli l_petallength=[] l_petalwidth=[] l_sepallength=[] l_sepalwidth=[] l_class=[] # array untuk data yang di normalisasi n_petallength=[] n_petalwidth=[] n_sepallength=[] n_sepalwidth=[] # fungsi untuk mengambil data dari file def mengambil_data(): for i in data: for j in data[i]: #print(type(j)) if(i == 'sepallength'): l_sepallength.append(j) elif(i =='sepalwidth'): l_sepalwidth.append(j) elif(i == 'petallength'): l_petallength.append(j) elif(i=='petalwidth'): l_petalwidth.append(j) for k in data['iris']: l_class.append(k) #normalisasi data def normalisasi(): for a in l_sepallength: value = (a-s.mean(l_sepallength))/s.stdev(l_sepallength) n_sepallength.append(value) for b in l_sepalwidth: value = (b-s.mean(l_sepalwidth))/s.stdev(l_sepalwidth) n_sepalwidth.append(value) for c in l_petallength: value = (c-s.mean(l_petallength))/s.stdev(l_petallength) n_petallength.append(value) for d in l_petalwidth: value = (d-s.mean(l_petalwidth))/s.stdev(l_petalwidth) n_petalwidth.append(value) #list untuk menampung nilai yang sudah di normalisasi petallength=[] petalwidth=[] sepallength=[] sepalwidth=[] r_class=[] data_sample=[] tes_petallength=[] tes_petalwidth=[] tes_sepallength=[] tes_sepalwidth=[] tes_class=[] data_tes=[] #mengkategori data antara data sample dan data tes def pengelompokan(): for i in range(len(l_class)): #data asli if i < 25 : petallength.append(n_petallength[i]) elif i < 50 : petalwidth.append(n_petalwidth[i]) elif i < 75 : sepallength.append(n_sepallength[i]) elif i < 100 : sepalwidth.append(n_sepalwidth[i]) #r_class.append(l_class[i]) #data uji elif i <115: tes_petallength.append(n_petallength[i]) elif i <130: tes_petalwidth.append(n_petalwidth[i]) elif i <140: tes_sepallength.append(n_sepallength[i]) elif i <150: tes_sepalwidth.append(n_sepalwidth[i]) #tes_class.append(l_class[i]) #mengisi nilai pada data sample for a in petallength: data_sample.append(a) for b in petalwidth: data_sample.append(b) for c in sepalwidth: data_sample.append(c) for d in sepallength: data_sample.append(d) #mengisi nilai pada data tes for e in tes_petallength: data_tes.append(e) for f in tes_petalwidth: data_tes.append(f) for g in tes_sepalwidth: data_tes.append(g) for h in tes_sepallength: data_tes.append(h) #list yang menyimpan nilai jarak euclidean distance=[] #fungsi untuk knn def knn(): hasil = [] for i in range(len(data_tes)): for j in range(len(data_sample)): #menggunakan eucludean distance jarak = sqrt(pow(data_tes[i]-data_sample[j],2)) distance.append(jarak) #fungsi untuk weight knn def wknn(): frekuensi0=[] frekuensi1=[] jarak = [] for j in range(len(distance)): value = distance[j]/distance[k+1] jarak.append(value) for i in jarak: if i < 1: frekuensi0.append(i) elif i > 1: frekuensi1.append(i) print(\"jumlah frekuensi 0 = \",len(frekuensi0)) print(\"jumlah frekuensi 1 = \",len(frekuensi1)) #urutan eksekusi fungsi diatas mengambil_data() normalisasi() pengelompokan() knn() wknn() Hasil run : saya masukkan dengan nilai K = 7 : masukkan nilai k = 7 jumlah frekuensi 0 = 4363 jumlah frekuensi 1 = 630","title":"Tugas 5  K NN"},{"location":"Tugas 5  K-NN/#k-nn","text":"Algoritma K-Nearest Neighbor (K-NN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN .","title":"K-NN"},{"location":"Tugas 5  K-NN/#tahapan-langkah-algoritma-k-nn","text":"Menentukan parameter k (jumlah tetangga paling dekat). Menghitung kuadrat jarak eucliden objek terhadap data training yang diberikan. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah) Mengumpulkan kategori Y (Klasifikasi nearest neighbor berdasarkan nilai k) Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat dipredisikan kategori objek.","title":"Tahapan Langkah Algoritma K-NN"},{"location":"Tugas 5  K-NN/#contoh-kasus-perhitungan-k-nn","text":"Terdapat beberapa data yang berasal dari survey questioner tentang klasifikasi kualitas kertas tissue apakah baik atau jelek, dengan objek training dibawah ini menggunakan dua attribute yaitu daya tahan terhadap asam dan kekuatan. contoh perhitungan k-nn Akan diproduksi kembali kertas tissue dengan attribute X1=7 dan X2=4 , tanpa harus mengeluarkan biaya untuk melakukan survey, maka dapat diklasifikasikan kertas tissue tersebut termasuk yang baik atau jelek. contoh perhitungan k-nn (2) contoh perhitungan k-nn (3) Dengan mengurutkan jarak terkecil, semisal diambil K=3, maka perbandingan nya adalah 2 (Baik) >1 (Jelek). Maka dapat disimpulkan kertas tissue dengan attribute X1=7 dan X2=4 masuk ke kelas Baik.","title":"Contoh Kasus Perhitungan K-NN\u00b6"},{"location":"Tugas 5  K-NN/#contoh-kasus","text":"sepallength sepalwidth petallength petalwidth iris sepal length stat 5.1 3.5 1.4 0.2 Iris-setosa Count 150 4.9 3 1.4 0.2 Iris-setosa Minimum 4.3 4.7 3.2 1.3 0.2 Iris-setosa Maximum 7.9 4.6 3.1 1.5 0.2 Iris-setosa Mean 5.84 5 3.6 1.4 0.2 Iris-setosa Median 5.8 5.4 3.9 1.7 0.4 Iris-setosa Mode 5 4.6 3.4 1.4 0.3 Iris-setosa Quartile 1 5.1 5 3.4 1.5 0.2 Iris-setosa Range 3.6 4.4 2.9 1.4 0.2 Iris-setosa Variance 0.69 4.9 3.1 1.5 0.1 Iris-setosa Standard Deviation 0.83 5.4 3.7 1.5 0.2 Iris-setosa Coefficient of Variation 14.20% 4.8 3.4 1.6 0.2 Iris-setosa Skewness 0.31 4.8 3 1.4 0.1 Iris-setosa Kurtosis -0.55 4.3 3 1.1 0.1 Iris-setosa 5.8 4 1.2 0.2 Iris-setosa 5.7 4.4 1.5 0.4 Iris-setosa 5.4 3.9 1.3 0.4 Iris-setosa 5.1 3.5 1.4 0.3 Iris-setosa 5.7 3.8 1.7 0.3 Iris-setosa 5.1 3.8 1.5 0.3 Iris-setosa 5.4 3.4 1.7 0.2 Iris-setosa 5.1 3.7 1.5 0.4 Iris-setosa 4.6 3.6 1 0.2 Iris-setosa 5.1 3.3 1.7 0.5 Iris-setosa 4.8 3.4 1.9 0.2 Iris-setosa 5 3 1.6 0.2 Iris-setosa 5 3.4 1.6 0.4 Iris-setosa 5.2 3.5 1.5 0.2 Iris-setosa 5.2 3.4 1.4 0.2 Iris-setosa 4.7 3.2 1.6 0.2 Iris-setosa 4.8 3.1 1.6 0.2 Iris-setosa 5.4 3.4 1.5 0.4 Iris-setosa 5.2 4.1 1.5 0.1 Iris-setosa 5.5 4.2 1.4 0.2 Iris-setosa 4.9 3.1 1.5 0.1 Iris-setosa 5 3.2 1.2 0.2 Iris-setosa 5.5 3.5 1.3 0.2 Iris-setosa 4.9 3.1 1.5 0.1 Iris-setosa 4.4 3 1.3 0.2 Iris-setosa 5.1 3.4 1.5 0.2 Iris-setosa 5 3.5 1.3 0.3 Iris-setosa 4.5 2.3 1.3 0.3 Iris-setosa 4.4 3.2 1.3 0.2 Iris-setosa 5 3.5 1.6 0.6 Iris-setosa 5.1 3.8 1.9 0.4 Iris-setosa 4.8 3 1.4 0.3 Iris-setosa 5.1 3.8 1.6 0.2 Iris-setosa 4.6 3.2 1.4 0.2 Iris-setosa 5.3 3.7 1.5 0.2 Iris-setosa 5 3.3 1.4 0.2 Iris-setosa 7 3.2 4.7 1.4 Iris-versicolor 6.4 3.2 4.5 1.5 Iris-versicolor 6.9 3.1 4.9 1.5 Iris-versicolor 5.5 2.3 4 1.3 Iris-versicolor 6.5 2.8 4.6 1.5 Iris-versicolor 5.7 2.8 4.5 1.3 Iris-versicolor 6.3 3.3 4.7 1.6 Iris-versicolor 4.9 2.4 3.3 1 Iris-versicolor 6.6 2.9 4.6 1.3 Iris-versicolor 5.2 2.7 3.9 1.4 Iris-versicolor 5 2 3.5 1 Iris-versicolor 5.9 3 4.2 1.5 Iris-versicolor 6 2.2 4 1 Iris-versicolor 6.1 2.9 4.7 1.4 Iris-versicolor 5.6 2.9 3.6 1.3 Iris-versicolor 6.7 3.1 4.4 1.4 Iris-versicolor 5.6 3 4.5 1.5 Iris-versicolor 5.8 2.7 4.1 1 Iris-versicolor 6.2 2.2 4.5 1.5 Iris-versicolor 5.6 2.5 3.9 1.1 Iris-versicolor 5.9 3.2 4.8 1.8 Iris-versicolor 6.1 2.8 4 1.3 Iris-versicolor 6.3 2.5 4.9 1.5 Iris-versicolor 6.1 2.8 4.7 1.2 Iris-versicolor 6.4 2.9 4.3 1.3 Iris-versicolor 6.6 3 4.4 1.4 Iris-versicolor 6.8 2.8 4.8 1.4 Iris-versicolor 6.7 3 5 1.7 Iris-versicolor 6 2.9 4.5 1.5 Iris-versicolor 5.7 2.6 3.5 1 Iris-versicolor 5.5 2.4 3.8 1.1 Iris-versicolor 5.5 2.4 3.7 1 Iris-versicolor 5.8 2.7 3.9 1.2 Iris-versicolor 6 2.7 5.1 1.6 Iris-versicolor 5.4 3 4.5 1.5 Iris-versicolor 6 3.4 4.5 1.6 Iris-versicolor 6.7 3.1 4.7 1.5 Iris-versicolor 6.3 2.3 4.4 1.3 Iris-versicolor 5.6 3 4.1 1.3 Iris-versicolor 5.5 2.5 4 1.3 Iris-versicolor 5.5 2.6 4.4 1.2 Iris-versicolor 6.1 3 4.6 1.4 Iris-versicolor 5.8 2.6 4 1.2 Iris-versicolor 5 2.3 3.3 1 Iris-versicolor 5.6 2.7 4.2 1.3 Iris-versicolor 5.7 3 4.2 1.2 Iris-versicolor 5.7 2.9 4.2 1.3 Iris-versicolor 6.2 2.9 4.3 1.3 Iris-versicolor 5.1 2.5 3 1.1 Iris-versicolor 5.7 2.8 4.1 1.3 Iris-versicolor 6.3 3.3 6 2.5 Iris-virginica 5.8 2.7 5.1 1.9 Iris-virginica 7.1 3 5.9 2.1 Iris-virginica 6.3 2.9 5.6 1.8 Iris-virginica 6.5 3 5.8 2.2 Iris-virginica 7.6 3 6.6 2.1 Iris-virginica 4.9 2.5 4.5 1.7 Iris-virginica 7.3 2.9 6.3 1.8 Iris-virginica 6.7 2.5 5.8 1.8 Iris-virginica 7.2 3.6 6.1 2.5 Iris-virginica 6.5 3.2 5.1 2 Iris-virginica 6.4 2.7 5.3 1.9 Iris-virginica 6.8 3 5.5 2.1 Iris-virginica 5.7 2.5 5 2 Iris-virginica 5.8 2.8 5.1 2.4 Iris-virginica 6.4 3.2 5.3 2.3 Iris-virginica 6.5 3 5.5 1.8 Iris-virginica 7.7 3.8 6.7 2.2 Iris-virginica 7.7 2.6 6.9 2.3 Iris-virginica 6 2.2 5 1.5 Iris-virginica 6.9 3.2 5.7 2.3 Iris-virginica 5.6 2.8 4.9 2 Iris-virginica 7.7 2.8 6.7 2 Iris-virginica 6.3 2.7 4.9 1.8 Iris-virginica 6.7 3.3 5.7 2.1 Iris-virginica 7.2 3.2 6 1.8 Iris-virginica 6.2 2.8 4.8 1.8 Iris-virginica 6.1 3 4.9 1.8 Iris-virginica 6.4 2.8 5.6 2.1 Iris-virginica 7.2 3 5.8 1.6 Iris-virginica 7.4 2.8 6.1 1.9 Iris-virginica 7.9 3.8 6.4 2 Iris-virginica 6.4 2.8 5.6 2.2 Iris-virginica 6.3 2.8 5.1 1.5 Iris-virginica 6.1 2.6 5.6 1.4 Iris-virginica 7.7 3 6.1 2.3 Iris-virginica 6.3 3.4 5.6 2.4 Iris-virginica 6.4 3.1 5.5 1.8 Iris-virginica 6 3 4.8 1.8 Iris-virginica 6.9 3.1 5.4 2.1 Iris-virginica 6.7 3.1 5.6 2.4 Iris-virginica 6.9 3.1 5.1 2.3 Iris-virginica 5.8 2.7 5.1 1.9 Iris-virginica 6.8 3.2 5.9 2.3 Iris-virginica 6.7 3.3 5.7 2.5 Iris-virginica 6.7 3 5.2 2.3 Iris-virginica 6.3 2.5 5 1.9 Iris-virginica 6.5 3 5.2 2 Iris-virginica 6.2 3.4 5.4 2.3 Iris-virginica 5.9 3 5.1 1.8 Iris-virginica Berikut adalah Code dari KNN dari data yang telah saya buat #tugas k-nn import numpy as np import seaborn as sea import matplotlib.pyplot as mat from scipy import stats import pandas as calldata import math from math import sqrt import statistics as s # membaca data data = calldata.read_csv(\"Iris_tugas5.csv\") #print(data) #data uji # nilai k k = int(input(\"masukkan nilai k = \")) #termasuk class apa data diatas ? # array untuk menampung data dari data asli l_petallength=[] l_petalwidth=[] l_sepallength=[] l_sepalwidth=[] l_class=[] # array untuk data yang di normalisasi n_petallength=[] n_petalwidth=[] n_sepallength=[] n_sepalwidth=[] # fungsi untuk mengambil data dari file def mengambil_data(): for i in data: for j in data[i]: #print(type(j)) if(i == 'sepallength'): l_sepallength.append(j) elif(i =='sepalwidth'): l_sepalwidth.append(j) elif(i == 'petallength'): l_petallength.append(j) elif(i=='petalwidth'): l_petalwidth.append(j) for k in data['iris']: l_class.append(k) #normalisasi data def normalisasi(): for a in l_sepallength: value = (a-s.mean(l_sepallength))/s.stdev(l_sepallength) n_sepallength.append(value) for b in l_sepalwidth: value = (b-s.mean(l_sepalwidth))/s.stdev(l_sepalwidth) n_sepalwidth.append(value) for c in l_petallength: value = (c-s.mean(l_petallength))/s.stdev(l_petallength) n_petallength.append(value) for d in l_petalwidth: value = (d-s.mean(l_petalwidth))/s.stdev(l_petalwidth) n_petalwidth.append(value) #list untuk menampung nilai yang sudah di normalisasi petallength=[] petalwidth=[] sepallength=[] sepalwidth=[] r_class=[] data_sample=[] tes_petallength=[] tes_petalwidth=[] tes_sepallength=[] tes_sepalwidth=[] tes_class=[] data_tes=[] #mengkategori data antara data sample dan data tes def pengelompokan(): for i in range(len(l_class)): #data asli if i < 25 : petallength.append(n_petallength[i]) elif i < 50 : petalwidth.append(n_petalwidth[i]) elif i < 75 : sepallength.append(n_sepallength[i]) elif i < 100 : sepalwidth.append(n_sepalwidth[i]) #r_class.append(l_class[i]) #data uji elif i <115: tes_petallength.append(n_petallength[i]) elif i <130: tes_petalwidth.append(n_petalwidth[i]) elif i <140: tes_sepallength.append(n_sepallength[i]) elif i <150: tes_sepalwidth.append(n_sepalwidth[i]) #tes_class.append(l_class[i]) #mengisi nilai pada data sample for a in petallength: data_sample.append(a) for b in petalwidth: data_sample.append(b) for c in sepalwidth: data_sample.append(c) for d in sepallength: data_sample.append(d) #mengisi nilai pada data tes for e in tes_petallength: data_tes.append(e) for f in tes_petalwidth: data_tes.append(f) for g in tes_sepalwidth: data_tes.append(g) for h in tes_sepallength: data_tes.append(h) #list yang menyimpan nilai jarak euclidean distance=[] #fungsi untuk knn def knn(): hasil = [] for i in range(len(data_tes)): for j in range(len(data_sample)): #menggunakan eucludean distance jarak = sqrt(pow(data_tes[i]-data_sample[j],2)) distance.append(jarak) #fungsi untuk weight knn def wknn(): frekuensi0=[] frekuensi1=[] jarak = [] for j in range(len(distance)): value = distance[j]/distance[k+1] jarak.append(value) for i in jarak: if i < 1: frekuensi0.append(i) elif i > 1: frekuensi1.append(i) print(\"jumlah frekuensi 0 = \",len(frekuensi0)) print(\"jumlah frekuensi 1 = \",len(frekuensi1)) #urutan eksekusi fungsi diatas mengambil_data() normalisasi() pengelompokan() knn() wknn() Hasil run : saya masukkan dengan nilai K = 7 : masukkan nilai k = 7 jumlah frekuensi 0 = 4363 jumlah frekuensi 1 = 630","title":"Contoh kasus"}]}